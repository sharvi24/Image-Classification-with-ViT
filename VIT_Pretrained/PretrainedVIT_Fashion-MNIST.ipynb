{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CgJyHZWTnp7k",
    "outputId": "9c30a741-a818-4bd0-ee39-d7e393488fd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jvatf0Ndnwja"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/CS444\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41564PoUlIOl"
   },
   "outputs": [],
   "source": [
    "# installing libraries\n",
    "#!pip install torch\n",
    "#!pip install torchvision\n",
    "#!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vQmN-D2okUmU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import timm\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import model_selection, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6jTxGqZkYfS",
    "outputId": "f7174190-cc94-4591-c073-154e6fefd524"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vSmJeUwc_nd"
   },
   "source": [
    "## MNIST Pretrained - 40 eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RO09IiLFnOIC"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = (\"/content/drive/MyDrive/CS444/jx_vit_base_p16_224-80ecf9dd.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GysE0Vf9dDJ8"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.01\n",
    "N_EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HkdkbXuLF-HM"
   },
   "outputs": [],
   "source": [
    "# create image augmentations\n",
    "transforms_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NRWCTmD3GuvB"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "DOWNLOAD_PATH = '/data/mnist'\n",
    "train_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=True, download=True,\n",
    "                                       transform=transforms_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=False, download=True,\n",
    "                                      transform=transforms_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448,
     "referenced_widgets": [
      "26068d363c5b411cba6e8a07eb0eb335",
      "a9f79e02663b4b3a9624c4a4d30218ba",
      "2f840aa0f4f44e518b8a644aebcd754d",
      "1411a8a97d864de587e96fc3644834ae",
      "d50974ca5be6429dafc3a20e4d63bb3d",
      "35e96b7186764c07b620f68abb5e2b2e",
      "37285a82fc024ea493cf3619d88003cc",
      "46847522ffd24588bb4fec633bd7c3b1",
      "201b2082379744458a31a9665bd99f27",
      "df95cdea5bd144df8b113f048cd2db40",
      "7364e5a2823e4058926254a187c93681"
     ]
    },
    "id": "s-hbEUNcKpWh",
    "outputId": "776391a0-fcf6-4223-c257-caea86944b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./../datasets/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26068d363c5b411cba6e8a07eb0eb335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./../datasets/cifar-10-python.tar.gz to ./../datasets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ad2f9e275c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./../datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./../datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Files already downloaded and verified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Extracting {archive} to {extract_root}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mextract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mextract_archive\u001b[0;34m(from_path, to_path, remove_finished)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ARCHIVE_EXTRACTORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marchive_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremove_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_extract_tar\u001b[0;34m(from_path, to_path, compression)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_extract_tar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"r:{compression[1:]}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0;31m# Do not set_attrs directories, as we will do that further down\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m             self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n\u001b[0;32m-> 2002\u001b[0;31m                          numeric_owner=numeric_owner)\n\u001b[0m\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m         \u001b[0;31m# Reverse sort directories.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2042\u001b[0m             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n\u001b[1;32m   2043\u001b[0m                                  \u001b[0mset_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                                  numeric_owner=numeric_owner)\n\u001b[0m\u001b[1;32m   2045\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorlevel\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mmakefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2161\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2163\u001b[0;31m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmakeunknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import torchvision.datasets as datasets\n",
    "#from torch.utils.data import DataLoader\n",
    "#train_set = datasets.CIFAR10(root='./../datasets', train=True, download=True, transform=transforms_train)\n",
    "#test_set = datasets.CIFAR10(root='./../datasets', train=False, download=True, transform=transforms_test)\n",
    "#train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "#test_loader = DataLoader(test_set, BATCH_SIZE*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BYHIPJM5d2dq"
   },
   "outputs": [],
   "source": [
    "class ViTBase16(nn.Module):\n",
    "    def __init__(self, n_classes, pretrained=False):\n",
    "\n",
    "        super(ViTBase16, self).__init__()\n",
    "\n",
    "        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained=False)\n",
    "        if pretrained:\n",
    "            self.model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n",
    "        # keep track of training loss\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        self.model.train()\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if device.type == \"cuda\":\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = self.forward(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # Calculate Accuracy\n",
    "            accuracy = (output.argmax(dim=1) == target).float().mean()\n",
    "            # update training loss and accuracy\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                    print(f\"\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss}\")\n",
    "                \n",
    "\n",
    "        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\n",
    "\n",
    "    def validate_one_epoch(self, test_loader, criterion, device):\n",
    "        # keep track of validation loss\n",
    "        valid_loss = 0.0\n",
    "        valid_accuracy = 0.0\n",
    "\n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        self.model.eval()\n",
    "        for data, target in test_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if device.type == \"cuda\":\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = self.model(data)\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "                # Calculate Accuracy\n",
    "                accuracy = (output.argmax(dim=1) == target).float().mean()\n",
    "                # update average validation loss and accuracy\n",
    "                valid_loss += loss\n",
    "                valid_accuracy += accuracy\n",
    "\n",
    "        return valid_loss / len(test_loader), valid_accuracy / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F7Wlso0ueFTD"
   },
   "outputs": [],
   "source": [
    "def fit_tpu(\n",
    "    model, epochs, device, criterion, optimizer, train_loader, test_loader=None\n",
    "):\n",
    "\n",
    "    valid_loss_min = np.Inf  # track change in validation loss\n",
    "\n",
    "    # keeping track of losses as it happen\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"EPOCH {epoch} - TRAINING...\")\n",
    "        train_loss, train_acc = model.train_one_epoch(\n",
    "            train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        print(\n",
    "            f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\n\"\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        gc.collect()\n",
    "\n",
    "        if test_loader is not None:\n",
    "            gc.collect()\n",
    "            print(f\"EPOCH {epoch} - VALIDATING...\")\n",
    "            valid_loss, valid_acc = model.validate_one_epoch(\n",
    "                test_loader, criterion, device\n",
    "            )\n",
    "            print(f\"\\t[VALID] LOSS: {valid_loss}, ACCURACY: {valid_acc}\\n\")\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_accs.append(valid_acc)\n",
    "            gc.collect()\n",
    "\n",
    "            # save model if validation loss has decreased\n",
    "            if valid_loss <= valid_loss_min and epoch != 1:\n",
    "                print(\n",
    "                    \"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\".format(\n",
    "                        valid_loss_min, valid_loss\n",
    "                    )\n",
    "                )\n",
    "            #                 xm.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"valid_losses\": valid_losses,\n",
    "        \"train_acc\": train_accs,\n",
    "        \"valid_acc\": valid_accs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4o2PUpUheJCP"
   },
   "outputs": [],
   "source": [
    "model = ViTBase16(n_classes=10, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lsoRP-gGHReT",
    "outputId": "501dedb0-35cf-450c-d785-046a829de073"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING TRAINING\n",
      "Start Time: 2022-05-12 18:01:51.143416\n",
      "==================================================\n",
      "EPOCH 1 - TRAINING...\n",
      "\tBATCH 1/3750 - LOSS: 2.2490615844726562\n",
      "\tBATCH 21/3750 - LOSS: 2.7702512741088867\n",
      "\tBATCH 41/3750 - LOSS: 1.9304150342941284\n",
      "\tBATCH 61/3750 - LOSS: 2.0892181396484375\n",
      "\tBATCH 81/3750 - LOSS: 1.778267741203308\n",
      "\tBATCH 101/3750 - LOSS: 2.038327932357788\n",
      "\tBATCH 121/3750 - LOSS: 1.8042110204696655\n",
      "\tBATCH 141/3750 - LOSS: 2.1231768131256104\n",
      "\tBATCH 161/3750 - LOSS: 2.247530698776245\n",
      "\tBATCH 181/3750 - LOSS: 1.9126079082489014\n",
      "\tBATCH 201/3750 - LOSS: 1.871691107749939\n",
      "\tBATCH 221/3750 - LOSS: 1.751727819442749\n",
      "\tBATCH 241/3750 - LOSS: 1.8868409395217896\n",
      "\tBATCH 261/3750 - LOSS: 1.8334274291992188\n",
      "\tBATCH 281/3750 - LOSS: 2.15313720703125\n",
      "\tBATCH 301/3750 - LOSS: 1.9897031784057617\n",
      "\tBATCH 321/3750 - LOSS: 1.792114496231079\n",
      "\tBATCH 341/3750 - LOSS: 2.077228307723999\n",
      "\tBATCH 361/3750 - LOSS: 1.7914998531341553\n",
      "\tBATCH 381/3750 - LOSS: 1.9459574222564697\n",
      "\tBATCH 401/3750 - LOSS: 1.9408681392669678\n",
      "\tBATCH 421/3750 - LOSS: 1.9015637636184692\n",
      "\tBATCH 441/3750 - LOSS: 1.8606175184249878\n",
      "\tBATCH 461/3750 - LOSS: 1.7614755630493164\n",
      "\tBATCH 481/3750 - LOSS: 2.0731608867645264\n",
      "\tBATCH 501/3750 - LOSS: 2.1855573654174805\n",
      "\tBATCH 521/3750 - LOSS: 1.9924099445343018\n",
      "\tBATCH 541/3750 - LOSS: 1.8156228065490723\n",
      "\tBATCH 561/3750 - LOSS: 1.7499626874923706\n",
      "\tBATCH 581/3750 - LOSS: 2.2070388793945312\n",
      "\tBATCH 601/3750 - LOSS: 2.2516210079193115\n",
      "\tBATCH 621/3750 - LOSS: 1.8777457475662231\n",
      "\tBATCH 641/3750 - LOSS: 2.0828239917755127\n",
      "\tBATCH 661/3750 - LOSS: 1.9769577980041504\n",
      "\tBATCH 681/3750 - LOSS: 1.79781174659729\n",
      "\tBATCH 701/3750 - LOSS: 2.1681907176971436\n",
      "\tBATCH 721/3750 - LOSS: 2.0190346240997314\n",
      "\tBATCH 741/3750 - LOSS: 1.3008893728256226\n",
      "\tBATCH 761/3750 - LOSS: 2.053987741470337\n",
      "\tBATCH 781/3750 - LOSS: 2.0463478565216064\n",
      "\tBATCH 801/3750 - LOSS: 1.9895131587982178\n",
      "\tBATCH 821/3750 - LOSS: 2.1929101943969727\n",
      "\tBATCH 841/3750 - LOSS: 1.4847460985183716\n",
      "\tBATCH 861/3750 - LOSS: 2.8850855827331543\n",
      "\tBATCH 881/3750 - LOSS: 2.0508272647857666\n",
      "\tBATCH 901/3750 - LOSS: 2.191709518432617\n",
      "\tBATCH 921/3750 - LOSS: 1.6619757413864136\n",
      "\tBATCH 941/3750 - LOSS: 2.093142032623291\n",
      "\tBATCH 961/3750 - LOSS: 1.9156293869018555\n",
      "\tBATCH 981/3750 - LOSS: 1.480238437652588\n",
      "\tBATCH 1001/3750 - LOSS: 1.6440328359603882\n",
      "\tBATCH 1021/3750 - LOSS: 1.693708896636963\n",
      "\tBATCH 1041/3750 - LOSS: 2.3315796852111816\n",
      "\tBATCH 1061/3750 - LOSS: 2.272469997406006\n",
      "\tBATCH 1081/3750 - LOSS: 2.2570955753326416\n",
      "\tBATCH 1101/3750 - LOSS: 1.851813793182373\n",
      "\tBATCH 1121/3750 - LOSS: 1.8267772197723389\n",
      "\tBATCH 1141/3750 - LOSS: 2.349364757537842\n",
      "\tBATCH 1161/3750 - LOSS: 2.02345871925354\n",
      "\tBATCH 1181/3750 - LOSS: 1.9423880577087402\n",
      "\tBATCH 1201/3750 - LOSS: 2.3213651180267334\n",
      "\tBATCH 1221/3750 - LOSS: 2.2111854553222656\n",
      "\tBATCH 1241/3750 - LOSS: 1.9817216396331787\n",
      "\tBATCH 1261/3750 - LOSS: 2.1040844917297363\n",
      "\tBATCH 1281/3750 - LOSS: 2.0177247524261475\n",
      "\tBATCH 1301/3750 - LOSS: 1.6293641328811646\n",
      "\tBATCH 1321/3750 - LOSS: 2.120215654373169\n",
      "\tBATCH 1341/3750 - LOSS: 2.259028434753418\n",
      "\tBATCH 1361/3750 - LOSS: 1.7023868560791016\n",
      "\tBATCH 1381/3750 - LOSS: 2.2337565422058105\n",
      "\tBATCH 1401/3750 - LOSS: 1.854296326637268\n",
      "\tBATCH 1421/3750 - LOSS: 2.151108980178833\n",
      "\tBATCH 1441/3750 - LOSS: 2.19712495803833\n",
      "\tBATCH 1461/3750 - LOSS: 2.2417843341827393\n",
      "\tBATCH 1481/3750 - LOSS: 1.8497599363327026\n",
      "\tBATCH 1501/3750 - LOSS: 1.5946320295333862\n",
      "\tBATCH 1521/3750 - LOSS: 1.949543833732605\n",
      "\tBATCH 1541/3750 - LOSS: 1.7353236675262451\n",
      "\tBATCH 1561/3750 - LOSS: 1.772269368171692\n",
      "\tBATCH 1581/3750 - LOSS: 1.9517817497253418\n",
      "\tBATCH 1601/3750 - LOSS: 2.389559507369995\n",
      "\tBATCH 1621/3750 - LOSS: 1.9675260782241821\n",
      "\tBATCH 1641/3750 - LOSS: 1.9047425985336304\n",
      "\tBATCH 1661/3750 - LOSS: 1.9739762544631958\n",
      "\tBATCH 1681/3750 - LOSS: 2.483191967010498\n",
      "\tBATCH 1701/3750 - LOSS: 1.9199519157409668\n",
      "\tBATCH 1721/3750 - LOSS: 2.1313693523406982\n",
      "\tBATCH 1741/3750 - LOSS: 2.297670602798462\n",
      "\tBATCH 1761/3750 - LOSS: 2.137014389038086\n",
      "\tBATCH 1781/3750 - LOSS: 2.6401281356811523\n",
      "\tBATCH 1801/3750 - LOSS: 2.5060720443725586\n",
      "\tBATCH 1821/3750 - LOSS: 2.0968194007873535\n",
      "\tBATCH 1841/3750 - LOSS: 1.7503894567489624\n",
      "\tBATCH 1861/3750 - LOSS: 1.9227608442306519\n",
      "\tBATCH 1881/3750 - LOSS: 1.8956809043884277\n",
      "\tBATCH 1901/3750 - LOSS: 2.3346030712127686\n",
      "\tBATCH 1921/3750 - LOSS: 1.9383755922317505\n",
      "\tBATCH 1941/3750 - LOSS: 2.5610015392303467\n",
      "\tBATCH 1961/3750 - LOSS: 2.164051055908203\n",
      "\tBATCH 1981/3750 - LOSS: 2.044267177581787\n",
      "\tBATCH 2001/3750 - LOSS: 2.26019024848938\n",
      "\tBATCH 2021/3750 - LOSS: 1.5735585689544678\n",
      "\tBATCH 2041/3750 - LOSS: 2.0996274948120117\n",
      "\tBATCH 2061/3750 - LOSS: 1.7551543712615967\n",
      "\tBATCH 2081/3750 - LOSS: 1.955674171447754\n",
      "\tBATCH 2101/3750 - LOSS: 1.709498405456543\n",
      "\tBATCH 2121/3750 - LOSS: 2.120157241821289\n",
      "\tBATCH 2141/3750 - LOSS: 1.9547770023345947\n",
      "\tBATCH 2161/3750 - LOSS: 1.6155167818069458\n",
      "\tBATCH 2181/3750 - LOSS: 1.8642687797546387\n",
      "\tBATCH 2201/3750 - LOSS: 2.0909581184387207\n",
      "\tBATCH 2221/3750 - LOSS: 2.5188381671905518\n",
      "\tBATCH 2241/3750 - LOSS: 1.8399665355682373\n",
      "\tBATCH 2261/3750 - LOSS: 1.8627450466156006\n",
      "\tBATCH 2281/3750 - LOSS: 1.2711131572723389\n",
      "\tBATCH 2301/3750 - LOSS: 2.078826904296875\n",
      "\tBATCH 2321/3750 - LOSS: 2.3635051250457764\n",
      "\tBATCH 2341/3750 - LOSS: 2.103426456451416\n",
      "\tBATCH 2361/3750 - LOSS: 1.666671872138977\n",
      "\tBATCH 2381/3750 - LOSS: 2.141122341156006\n",
      "\tBATCH 2401/3750 - LOSS: 2.2338438034057617\n",
      "\tBATCH 2421/3750 - LOSS: 2.0425593852996826\n",
      "\tBATCH 2441/3750 - LOSS: 1.5499190092086792\n",
      "\tBATCH 2461/3750 - LOSS: 1.7659791707992554\n",
      "\tBATCH 2481/3750 - LOSS: 1.5977346897125244\n",
      "\tBATCH 2501/3750 - LOSS: 1.778071641921997\n",
      "\tBATCH 2521/3750 - LOSS: 1.7838481664657593\n",
      "\tBATCH 2541/3750 - LOSS: 1.8385652303695679\n",
      "\tBATCH 2561/3750 - LOSS: 2.0544273853302\n",
      "\tBATCH 2581/3750 - LOSS: 1.5017837285995483\n",
      "\tBATCH 2601/3750 - LOSS: 1.921736717224121\n",
      "\tBATCH 2621/3750 - LOSS: 2.3356375694274902\n",
      "\tBATCH 2641/3750 - LOSS: 1.7233790159225464\n",
      "\tBATCH 2661/3750 - LOSS: 1.8998156785964966\n",
      "\tBATCH 2681/3750 - LOSS: 1.7381364107131958\n",
      "\tBATCH 2701/3750 - LOSS: 1.6062558889389038\n",
      "\tBATCH 2721/3750 - LOSS: 1.753831148147583\n",
      "\tBATCH 2741/3750 - LOSS: 1.6172990798950195\n",
      "\tBATCH 2761/3750 - LOSS: 1.956923246383667\n",
      "\tBATCH 2781/3750 - LOSS: 1.9218178987503052\n",
      "\tBATCH 2801/3750 - LOSS: 2.2951149940490723\n",
      "\tBATCH 2821/3750 - LOSS: 2.015751361846924\n",
      "\tBATCH 2841/3750 - LOSS: 1.7521648406982422\n",
      "\tBATCH 2861/3750 - LOSS: 1.5327773094177246\n",
      "\tBATCH 2881/3750 - LOSS: 1.8205441236495972\n",
      "\tBATCH 2901/3750 - LOSS: 1.9336448907852173\n",
      "\tBATCH 2921/3750 - LOSS: 1.6415095329284668\n",
      "\tBATCH 2941/3750 - LOSS: 1.947251319885254\n",
      "\tBATCH 2961/3750 - LOSS: 1.8350882530212402\n",
      "\tBATCH 2981/3750 - LOSS: 1.6425904035568237\n",
      "\tBATCH 3001/3750 - LOSS: 1.833490014076233\n",
      "\tBATCH 3021/3750 - LOSS: 1.9511438608169556\n",
      "\tBATCH 3041/3750 - LOSS: 2.173699378967285\n",
      "\tBATCH 3061/3750 - LOSS: 1.8931066989898682\n",
      "\tBATCH 3081/3750 - LOSS: 2.2860512733459473\n",
      "\tBATCH 3101/3750 - LOSS: 2.265646457672119\n",
      "\tBATCH 3121/3750 - LOSS: 2.5285463333129883\n",
      "\tBATCH 3141/3750 - LOSS: 2.407367706298828\n",
      "\tBATCH 3161/3750 - LOSS: 2.019765853881836\n",
      "\tBATCH 3181/3750 - LOSS: 1.9251599311828613\n",
      "\tBATCH 3201/3750 - LOSS: 2.2851216793060303\n",
      "\tBATCH 3221/3750 - LOSS: 2.351555347442627\n",
      "\tBATCH 3241/3750 - LOSS: 1.895708680152893\n",
      "\tBATCH 3261/3750 - LOSS: 2.041922092437744\n",
      "\tBATCH 3281/3750 - LOSS: 1.9628794193267822\n",
      "\tBATCH 3301/3750 - LOSS: 1.7376726865768433\n",
      "\tBATCH 3321/3750 - LOSS: 2.0615670680999756\n",
      "\tBATCH 3341/3750 - LOSS: 1.9405765533447266\n",
      "\tBATCH 3361/3750 - LOSS: 1.6277707815170288\n",
      "\tBATCH 3381/3750 - LOSS: 1.900759220123291\n",
      "\tBATCH 3401/3750 - LOSS: 2.2225658893585205\n",
      "\tBATCH 3421/3750 - LOSS: 1.8073623180389404\n",
      "\tBATCH 3441/3750 - LOSS: 1.5321491956710815\n",
      "\tBATCH 3461/3750 - LOSS: 1.6608893871307373\n",
      "\tBATCH 3481/3750 - LOSS: 2.1740305423736572\n",
      "\tBATCH 3501/3750 - LOSS: 1.9972912073135376\n",
      "\tBATCH 3521/3750 - LOSS: 2.1078648567199707\n",
      "\tBATCH 3541/3750 - LOSS: 2.05854868888855\n",
      "\tBATCH 3561/3750 - LOSS: 1.7375696897506714\n",
      "\tBATCH 3581/3750 - LOSS: 1.779563307762146\n",
      "\tBATCH 3601/3750 - LOSS: 1.9204275608062744\n",
      "\tBATCH 3621/3750 - LOSS: 1.6829426288604736\n",
      "\tBATCH 3641/3750 - LOSS: 2.276341438293457\n",
      "\tBATCH 3661/3750 - LOSS: 2.025163412094116\n",
      "\tBATCH 3681/3750 - LOSS: 1.8704824447631836\n",
      "\tBATCH 3701/3750 - LOSS: 2.070733070373535\n",
      "\tBATCH 3721/3750 - LOSS: 1.956934928894043\n",
      "\tBATCH 3741/3750 - LOSS: 1.9492391347885132\n",
      "\n",
      "\t[TRAIN] EPOCH 1 - LOSS: 1.9557422399520874, ACCURACY: 0.29428333044052124\n",
      "\n",
      "EPOCH 1 - VALIDATING...\n",
      "\t[VALID] LOSS: 2.0803539752960205, ACCURACY: 0.25\n",
      "\n",
      "==================================================\n",
      "EPOCH 2 - TRAINING...\n",
      "\tBATCH 1/3750 - LOSS: 1.9548161029815674\n",
      "\tBATCH 21/3750 - LOSS: 1.64110267162323\n",
      "\tBATCH 41/3750 - LOSS: 1.8204092979431152\n",
      "\tBATCH 61/3750 - LOSS: 2.0250844955444336\n",
      "\tBATCH 81/3750 - LOSS: 1.8696651458740234\n",
      "\tBATCH 101/3750 - LOSS: 2.005387544631958\n",
      "\tBATCH 121/3750 - LOSS: 1.8894333839416504\n",
      "\tBATCH 141/3750 - LOSS: 2.034383773803711\n",
      "\tBATCH 161/3750 - LOSS: 1.8601322174072266\n",
      "\tBATCH 181/3750 - LOSS: 1.9075384140014648\n",
      "\tBATCH 201/3750 - LOSS: 2.0963761806488037\n",
      "\tBATCH 221/3750 - LOSS: 2.5617117881774902\n",
      "\tBATCH 241/3750 - LOSS: 1.6475727558135986\n",
      "\tBATCH 261/3750 - LOSS: 2.268103837966919\n",
      "\tBATCH 281/3750 - LOSS: 2.0201902389526367\n",
      "\tBATCH 301/3750 - LOSS: 1.834936499595642\n",
      "\tBATCH 321/3750 - LOSS: 1.9246422052383423\n",
      "\tBATCH 341/3750 - LOSS: 1.9770889282226562\n",
      "\tBATCH 361/3750 - LOSS: 1.8866615295410156\n",
      "\tBATCH 381/3750 - LOSS: 1.939604640007019\n",
      "\tBATCH 401/3750 - LOSS: 1.6312811374664307\n",
      "\tBATCH 421/3750 - LOSS: 2.1773838996887207\n",
      "\tBATCH 441/3750 - LOSS: 2.2684783935546875\n",
      "\tBATCH 461/3750 - LOSS: 1.6424845457077026\n",
      "\tBATCH 481/3750 - LOSS: 1.9969234466552734\n",
      "\tBATCH 501/3750 - LOSS: 1.5171953439712524\n",
      "\tBATCH 521/3750 - LOSS: 1.9404107332229614\n",
      "\tBATCH 541/3750 - LOSS: 1.7588534355163574\n",
      "\tBATCH 561/3750 - LOSS: 1.8157868385314941\n",
      "\tBATCH 581/3750 - LOSS: 1.7064087390899658\n",
      "\tBATCH 601/3750 - LOSS: 1.6975650787353516\n",
      "\tBATCH 621/3750 - LOSS: 1.451972246170044\n",
      "\tBATCH 641/3750 - LOSS: 1.6599277257919312\n",
      "\tBATCH 661/3750 - LOSS: 1.7613694667816162\n",
      "\tBATCH 681/3750 - LOSS: 1.4336446523666382\n",
      "\tBATCH 701/3750 - LOSS: 2.281820297241211\n",
      "\tBATCH 721/3750 - LOSS: 1.2833380699157715\n",
      "\tBATCH 741/3750 - LOSS: 2.117072343826294\n",
      "\tBATCH 761/3750 - LOSS: 1.6556758880615234\n",
      "\tBATCH 781/3750 - LOSS: 2.028409719467163\n",
      "\tBATCH 801/3750 - LOSS: 1.6514719724655151\n",
      "\tBATCH 821/3750 - LOSS: 1.6941986083984375\n",
      "\tBATCH 841/3750 - LOSS: 2.040339469909668\n",
      "\tBATCH 861/3750 - LOSS: 1.9706614017486572\n",
      "\tBATCH 881/3750 - LOSS: 1.9727047681808472\n",
      "\tBATCH 901/3750 - LOSS: 1.820347785949707\n",
      "\tBATCH 921/3750 - LOSS: 2.055473566055298\n",
      "\tBATCH 941/3750 - LOSS: 1.8631691932678223\n",
      "\tBATCH 961/3750 - LOSS: 1.8934504985809326\n",
      "\tBATCH 981/3750 - LOSS: 2.2094922065734863\n",
      "\tBATCH 1001/3750 - LOSS: 1.9027907848358154\n",
      "\tBATCH 1021/3750 - LOSS: 2.189223289489746\n",
      "\tBATCH 1041/3750 - LOSS: 1.55678129196167\n",
      "\tBATCH 1061/3750 - LOSS: 2.142245292663574\n",
      "\tBATCH 1081/3750 - LOSS: 2.0665032863616943\n",
      "\tBATCH 1101/3750 - LOSS: 1.9432439804077148\n",
      "\tBATCH 1121/3750 - LOSS: 1.9463260173797607\n",
      "\tBATCH 1141/3750 - LOSS: 2.289149284362793\n",
      "\tBATCH 1161/3750 - LOSS: 1.9121146202087402\n",
      "\tBATCH 1181/3750 - LOSS: 2.086700677871704\n",
      "\tBATCH 1201/3750 - LOSS: 1.689260482788086\n",
      "\tBATCH 1221/3750 - LOSS: 2.0055177211761475\n",
      "\tBATCH 1241/3750 - LOSS: 2.29135799407959\n",
      "\tBATCH 1261/3750 - LOSS: 2.348736524581909\n",
      "\tBATCH 1281/3750 - LOSS: 2.163966655731201\n",
      "\tBATCH 1301/3750 - LOSS: 2.1687920093536377\n",
      "\tBATCH 1321/3750 - LOSS: 1.745071530342102\n",
      "\tBATCH 1341/3750 - LOSS: 2.2163915634155273\n",
      "\tBATCH 1361/3750 - LOSS: 2.254816770553589\n",
      "\tBATCH 1381/3750 - LOSS: 1.7715107202529907\n",
      "\tBATCH 1401/3750 - LOSS: 1.9813485145568848\n",
      "\tBATCH 1421/3750 - LOSS: 1.6848944425582886\n",
      "\tBATCH 1441/3750 - LOSS: 1.9097858667373657\n",
      "\tBATCH 1461/3750 - LOSS: 1.8305628299713135\n",
      "\tBATCH 1481/3750 - LOSS: 2.3935964107513428\n",
      "\tBATCH 1501/3750 - LOSS: 1.9021211862564087\n",
      "\tBATCH 1521/3750 - LOSS: 2.2538177967071533\n",
      "\tBATCH 1541/3750 - LOSS: 2.1071550846099854\n",
      "\tBATCH 1561/3750 - LOSS: 2.085832357406616\n",
      "\tBATCH 1581/3750 - LOSS: 2.0285844802856445\n",
      "\tBATCH 1601/3750 - LOSS: 1.5817211866378784\n",
      "\tBATCH 1621/3750 - LOSS: 1.9565538167953491\n",
      "\tBATCH 1641/3750 - LOSS: 1.7840155363082886\n",
      "\tBATCH 1661/3750 - LOSS: 1.9200756549835205\n",
      "\tBATCH 1681/3750 - LOSS: 1.706295132637024\n",
      "\tBATCH 1701/3750 - LOSS: 1.9066901206970215\n",
      "\tBATCH 1721/3750 - LOSS: 1.9648977518081665\n",
      "\tBATCH 1741/3750 - LOSS: 1.7949097156524658\n",
      "\tBATCH 1761/3750 - LOSS: 2.2677347660064697\n",
      "\tBATCH 1781/3750 - LOSS: 2.015986919403076\n",
      "\tBATCH 1801/3750 - LOSS: 1.9937949180603027\n",
      "\tBATCH 1821/3750 - LOSS: 1.9512065649032593\n",
      "\tBATCH 1841/3750 - LOSS: 1.9916869401931763\n",
      "\tBATCH 1861/3750 - LOSS: 2.1474502086639404\n",
      "\tBATCH 1881/3750 - LOSS: 2.422520637512207\n",
      "\tBATCH 1901/3750 - LOSS: 1.778486967086792\n",
      "\tBATCH 1921/3750 - LOSS: 2.2262966632843018\n",
      "\tBATCH 1941/3750 - LOSS: 1.8216156959533691\n",
      "\tBATCH 1961/3750 - LOSS: 1.7573078870773315\n",
      "\tBATCH 1981/3750 - LOSS: 1.9208635091781616\n",
      "\tBATCH 2001/3750 - LOSS: 1.773665189743042\n",
      "\tBATCH 2021/3750 - LOSS: 2.056075096130371\n",
      "\tBATCH 2041/3750 - LOSS: 2.040386199951172\n",
      "\tBATCH 2061/3750 - LOSS: 2.127040147781372\n",
      "\tBATCH 2081/3750 - LOSS: 2.0908048152923584\n",
      "\tBATCH 2101/3750 - LOSS: 2.0040979385375977\n",
      "\tBATCH 2121/3750 - LOSS: 1.7320239543914795\n",
      "\tBATCH 2141/3750 - LOSS: 1.8564953804016113\n",
      "\tBATCH 2161/3750 - LOSS: 2.0487546920776367\n",
      "\tBATCH 2181/3750 - LOSS: 1.7281650304794312\n",
      "\tBATCH 2201/3750 - LOSS: 2.1461381912231445\n",
      "\tBATCH 2221/3750 - LOSS: 2.10994291305542\n",
      "\tBATCH 2241/3750 - LOSS: 2.2656302452087402\n",
      "\tBATCH 2261/3750 - LOSS: 1.936981439590454\n",
      "\tBATCH 2281/3750 - LOSS: 2.3631930351257324\n",
      "\tBATCH 2301/3750 - LOSS: 1.776675820350647\n",
      "\tBATCH 2321/3750 - LOSS: 2.095393180847168\n",
      "\tBATCH 2341/3750 - LOSS: 1.8517985343933105\n",
      "\tBATCH 2361/3750 - LOSS: 2.220259666442871\n",
      "\tBATCH 2381/3750 - LOSS: 1.851072072982788\n",
      "\tBATCH 2401/3750 - LOSS: 1.748826265335083\n",
      "\tBATCH 2421/3750 - LOSS: 2.173914909362793\n",
      "\tBATCH 2441/3750 - LOSS: 1.9212422370910645\n",
      "\tBATCH 2461/3750 - LOSS: 2.1836466789245605\n",
      "\tBATCH 2481/3750 - LOSS: 2.116373300552368\n",
      "\tBATCH 2501/3750 - LOSS: 2.0506069660186768\n",
      "\tBATCH 2521/3750 - LOSS: 1.7643615007400513\n",
      "\tBATCH 2541/3750 - LOSS: 1.623780369758606\n",
      "\tBATCH 2561/3750 - LOSS: 2.059400796890259\n",
      "\tBATCH 2581/3750 - LOSS: 2.2155113220214844\n",
      "\tBATCH 2601/3750 - LOSS: 1.874108076095581\n",
      "\tBATCH 2621/3750 - LOSS: 1.8607147932052612\n",
      "\tBATCH 2641/3750 - LOSS: 2.1832292079925537\n",
      "\tBATCH 2661/3750 - LOSS: 2.0675711631774902\n",
      "\tBATCH 2681/3750 - LOSS: 1.6273984909057617\n",
      "\tBATCH 2701/3750 - LOSS: 2.0435678958892822\n",
      "\tBATCH 2721/3750 - LOSS: 2.0024423599243164\n",
      "\tBATCH 2741/3750 - LOSS: 1.8437225818634033\n",
      "\tBATCH 2761/3750 - LOSS: 2.2246487140655518\n",
      "\tBATCH 2781/3750 - LOSS: 1.8053536415100098\n",
      "\tBATCH 2801/3750 - LOSS: 1.8433629274368286\n",
      "\tBATCH 2821/3750 - LOSS: 2.0894131660461426\n",
      "\tBATCH 2841/3750 - LOSS: 1.8219941854476929\n",
      "\tBATCH 2861/3750 - LOSS: 2.2180800437927246\n",
      "\tBATCH 2881/3750 - LOSS: 1.8636894226074219\n",
      "\tBATCH 2901/3750 - LOSS: 2.110321044921875\n",
      "\tBATCH 2921/3750 - LOSS: 2.1876628398895264\n",
      "\tBATCH 2941/3750 - LOSS: 2.3791542053222656\n",
      "\tBATCH 2961/3750 - LOSS: 2.0463449954986572\n",
      "\tBATCH 2981/3750 - LOSS: 1.8516926765441895\n",
      "\tBATCH 3001/3750 - LOSS: 2.408635377883911\n",
      "\tBATCH 3021/3750 - LOSS: 2.0198440551757812\n",
      "\tBATCH 3041/3750 - LOSS: 2.1096930503845215\n",
      "\tBATCH 3061/3750 - LOSS: 1.8341809511184692\n",
      "\tBATCH 3081/3750 - LOSS: 2.0511741638183594\n",
      "\tBATCH 3101/3750 - LOSS: 1.7557646036148071\n",
      "\tBATCH 3121/3750 - LOSS: 1.9232114553451538\n",
      "\tBATCH 3141/3750 - LOSS: 1.7764514684677124\n",
      "\tBATCH 3161/3750 - LOSS: 1.6878941059112549\n",
      "\tBATCH 3181/3750 - LOSS: 2.0914576053619385\n",
      "\tBATCH 3201/3750 - LOSS: 1.7289501428604126\n",
      "\tBATCH 3221/3750 - LOSS: 1.7004303932189941\n",
      "\tBATCH 3241/3750 - LOSS: 2.410013198852539\n",
      "\tBATCH 3261/3750 - LOSS: 2.1587796211242676\n",
      "\tBATCH 3281/3750 - LOSS: 1.700779914855957\n",
      "\tBATCH 3301/3750 - LOSS: 1.5321725606918335\n",
      "\tBATCH 3321/3750 - LOSS: 1.830446720123291\n",
      "\tBATCH 3341/3750 - LOSS: 1.9165822267532349\n",
      "\tBATCH 3361/3750 - LOSS: 2.158949851989746\n",
      "\tBATCH 3381/3750 - LOSS: 1.9527575969696045\n",
      "\tBATCH 3401/3750 - LOSS: 1.6677688360214233\n",
      "\tBATCH 3421/3750 - LOSS: 1.9048982858657837\n",
      "\tBATCH 3441/3750 - LOSS: 1.92583429813385\n",
      "\tBATCH 3461/3750 - LOSS: 1.6351298093795776\n",
      "\tBATCH 3481/3750 - LOSS: 1.8930981159210205\n",
      "\tBATCH 3501/3750 - LOSS: 2.300668478012085\n",
      "\tBATCH 3521/3750 - LOSS: 1.5953720808029175\n",
      "\tBATCH 3541/3750 - LOSS: 2.1994266510009766\n",
      "\tBATCH 3561/3750 - LOSS: 1.6113381385803223\n",
      "\tBATCH 3581/3750 - LOSS: 1.8751479387283325\n",
      "\tBATCH 3601/3750 - LOSS: 1.9622281789779663\n",
      "\tBATCH 3621/3750 - LOSS: 2.486741304397583\n",
      "\tBATCH 3641/3750 - LOSS: 2.158067464828491\n",
      "\tBATCH 3661/3750 - LOSS: 1.3690241575241089\n",
      "\tBATCH 3681/3750 - LOSS: 1.7234834432601929\n",
      "\tBATCH 3701/3750 - LOSS: 2.157289981842041\n",
      "\tBATCH 3721/3750 - LOSS: 1.8218410015106201\n",
      "\tBATCH 3741/3750 - LOSS: 2.179213285446167\n",
      "\n",
      "\t[TRAIN] EPOCH 2 - LOSS: 1.9465795755386353, ACCURACY: 0.2815999984741211\n",
      "\n",
      "EPOCH 2 - VALIDATING...\n",
      "\t[VALID] LOSS: 2.2135262489318848, ACCURACY: 0.210099995136261\n",
      "\n",
      "==================================================\n",
      "EPOCH 3 - TRAINING...\n",
      "\tBATCH 1/3750 - LOSS: 2.4335427284240723\n",
      "\tBATCH 21/3750 - LOSS: 1.9534239768981934\n",
      "\tBATCH 41/3750 - LOSS: 2.3678348064422607\n",
      "\tBATCH 61/3750 - LOSS: 1.730634093284607\n",
      "\tBATCH 81/3750 - LOSS: 2.1551191806793213\n",
      "\tBATCH 101/3750 - LOSS: 1.9442214965820312\n",
      "\tBATCH 121/3750 - LOSS: 1.9685858488082886\n",
      "\tBATCH 141/3750 - LOSS: 1.7767828702926636\n",
      "\tBATCH 161/3750 - LOSS: 1.8854931592941284\n",
      "\tBATCH 181/3750 - LOSS: 1.9448672533035278\n",
      "\tBATCH 201/3750 - LOSS: 1.617812991142273\n",
      "\tBATCH 221/3750 - LOSS: 2.068873167037964\n",
      "\tBATCH 241/3750 - LOSS: 2.3989639282226562\n",
      "\tBATCH 261/3750 - LOSS: 1.7561931610107422\n",
      "\tBATCH 281/3750 - LOSS: 2.2658395767211914\n",
      "\tBATCH 301/3750 - LOSS: 2.014993906021118\n",
      "\tBATCH 321/3750 - LOSS: 2.0048108100891113\n",
      "\tBATCH 341/3750 - LOSS: 1.8090029954910278\n",
      "\tBATCH 361/3750 - LOSS: 1.8074073791503906\n",
      "\tBATCH 381/3750 - LOSS: 1.6931047439575195\n",
      "\tBATCH 401/3750 - LOSS: 2.4477345943450928\n",
      "\tBATCH 421/3750 - LOSS: 2.121695041656494\n",
      "\tBATCH 441/3750 - LOSS: 1.7192037105560303\n",
      "\tBATCH 461/3750 - LOSS: 1.8138401508331299\n",
      "\tBATCH 481/3750 - LOSS: 1.910846471786499\n",
      "\tBATCH 501/3750 - LOSS: 2.1346828937530518\n",
      "\tBATCH 521/3750 - LOSS: 1.6557862758636475\n",
      "\tBATCH 541/3750 - LOSS: 2.0526397228240967\n",
      "\tBATCH 561/3750 - LOSS: 1.6920640468597412\n",
      "\tBATCH 581/3750 - LOSS: 1.8813797235488892\n",
      "\tBATCH 601/3750 - LOSS: 1.8375730514526367\n",
      "\tBATCH 621/3750 - LOSS: 1.619452953338623\n",
      "\tBATCH 641/3750 - LOSS: 1.5492191314697266\n",
      "\tBATCH 661/3750 - LOSS: 1.9368354082107544\n",
      "\tBATCH 681/3750 - LOSS: 2.1434762477874756\n",
      "\tBATCH 701/3750 - LOSS: 2.0925211906433105\n",
      "\tBATCH 721/3750 - LOSS: 1.9908243417739868\n",
      "\tBATCH 741/3750 - LOSS: 1.7341358661651611\n",
      "\tBATCH 761/3750 - LOSS: 1.7133798599243164\n",
      "\tBATCH 781/3750 - LOSS: 1.8944940567016602\n",
      "\tBATCH 801/3750 - LOSS: 1.6520887613296509\n",
      "\tBATCH 821/3750 - LOSS: 1.9031569957733154\n",
      "\tBATCH 841/3750 - LOSS: 2.158047676086426\n",
      "\tBATCH 861/3750 - LOSS: 1.6666160821914673\n",
      "\tBATCH 881/3750 - LOSS: 2.2852914333343506\n",
      "\tBATCH 901/3750 - LOSS: 2.077012538909912\n",
      "\tBATCH 921/3750 - LOSS: 2.0872154235839844\n",
      "\tBATCH 941/3750 - LOSS: 1.9345953464508057\n",
      "\tBATCH 961/3750 - LOSS: 2.639725685119629\n",
      "\tBATCH 981/3750 - LOSS: 1.92899489402771\n",
      "\tBATCH 1001/3750 - LOSS: 1.9966330528259277\n",
      "\tBATCH 1021/3750 - LOSS: 1.4625946283340454\n",
      "\tBATCH 1041/3750 - LOSS: 1.7793759107589722\n",
      "\tBATCH 1061/3750 - LOSS: 1.88427734375\n",
      "\tBATCH 1081/3750 - LOSS: 1.315511703491211\n",
      "\tBATCH 1101/3750 - LOSS: 2.0070738792419434\n",
      "\tBATCH 1121/3750 - LOSS: 1.6175514459609985\n",
      "\tBATCH 1141/3750 - LOSS: 1.6234413385391235\n",
      "\tBATCH 1161/3750 - LOSS: 1.8520334959030151\n",
      "\tBATCH 1181/3750 - LOSS: 2.2090630531311035\n",
      "\tBATCH 1201/3750 - LOSS: 2.3335487842559814\n",
      "\tBATCH 1221/3750 - LOSS: 2.2085044384002686\n",
      "\tBATCH 1241/3750 - LOSS: 1.8357189893722534\n",
      "\tBATCH 1261/3750 - LOSS: 1.8187825679779053\n",
      "\tBATCH 1281/3750 - LOSS: 1.8888367414474487\n",
      "\tBATCH 1301/3750 - LOSS: 1.7316691875457764\n",
      "\tBATCH 1321/3750 - LOSS: 1.8237422704696655\n",
      "\tBATCH 1341/3750 - LOSS: 2.372633457183838\n",
      "\tBATCH 1361/3750 - LOSS: 2.1827213764190674\n",
      "\tBATCH 1381/3750 - LOSS: 2.093522071838379\n",
      "\tBATCH 1401/3750 - LOSS: 2.0471200942993164\n",
      "\tBATCH 1421/3750 - LOSS: 1.802133321762085\n",
      "\tBATCH 1441/3750 - LOSS: 2.0661697387695312\n",
      "\tBATCH 1461/3750 - LOSS: 2.016378164291382\n",
      "\tBATCH 1481/3750 - LOSS: 2.2618002891540527\n",
      "\tBATCH 1501/3750 - LOSS: 1.589942455291748\n",
      "\tBATCH 1521/3750 - LOSS: 2.2917051315307617\n",
      "\tBATCH 1541/3750 - LOSS: 2.1480865478515625\n",
      "\tBATCH 1561/3750 - LOSS: 2.071984052658081\n",
      "\tBATCH 1581/3750 - LOSS: 1.544365644454956\n",
      "\tBATCH 1601/3750 - LOSS: 1.781278371810913\n",
      "\tBATCH 1621/3750 - LOSS: 2.0186517238616943\n",
      "\tBATCH 1641/3750 - LOSS: 1.97579026222229\n",
      "\tBATCH 1661/3750 - LOSS: 1.7479753494262695\n",
      "\tBATCH 1681/3750 - LOSS: 2.3365116119384766\n",
      "\tBATCH 1701/3750 - LOSS: 1.8068487644195557\n",
      "\tBATCH 1721/3750 - LOSS: 2.1839704513549805\n",
      "\tBATCH 1741/3750 - LOSS: 1.8112101554870605\n",
      "\tBATCH 1761/3750 - LOSS: 1.8194772005081177\n",
      "\tBATCH 1781/3750 - LOSS: 2.090618371963501\n",
      "\tBATCH 1801/3750 - LOSS: 1.7502347230911255\n",
      "\tBATCH 1821/3750 - LOSS: 2.039052963256836\n",
      "\tBATCH 1841/3750 - LOSS: 2.2552590370178223\n",
      "\tBATCH 1861/3750 - LOSS: 1.9751698970794678\n",
      "\tBATCH 1881/3750 - LOSS: 1.9621318578720093\n",
      "\tBATCH 1901/3750 - LOSS: 1.799498200416565\n",
      "\tBATCH 1921/3750 - LOSS: 1.6015383005142212\n",
      "\tBATCH 1941/3750 - LOSS: 2.092458486557007\n",
      "\tBATCH 1961/3750 - LOSS: 1.9469094276428223\n",
      "\tBATCH 1981/3750 - LOSS: 2.0156495571136475\n",
      "\tBATCH 2001/3750 - LOSS: 2.2264909744262695\n",
      "\tBATCH 2021/3750 - LOSS: 2.175905227661133\n",
      "\tBATCH 2041/3750 - LOSS: 1.9177533388137817\n",
      "\tBATCH 2061/3750 - LOSS: 1.5351159572601318\n",
      "\tBATCH 2081/3750 - LOSS: 1.6752963066101074\n",
      "\tBATCH 2101/3750 - LOSS: 2.254046678543091\n",
      "\tBATCH 2121/3750 - LOSS: 2.095104455947876\n",
      "\tBATCH 2141/3750 - LOSS: 2.008105993270874\n",
      "\tBATCH 2161/3750 - LOSS: 2.3766565322875977\n",
      "\tBATCH 2181/3750 - LOSS: 2.1639089584350586\n",
      "\tBATCH 2201/3750 - LOSS: 2.171947479248047\n",
      "\tBATCH 2221/3750 - LOSS: 2.220731496810913\n",
      "\tBATCH 2241/3750 - LOSS: 2.067281723022461\n",
      "\tBATCH 2261/3750 - LOSS: 2.0050547122955322\n",
      "\tBATCH 2281/3750 - LOSS: 2.1162915229797363\n",
      "\tBATCH 2301/3750 - LOSS: 1.9222915172576904\n",
      "\tBATCH 2321/3750 - LOSS: 1.6116160154342651\n",
      "\tBATCH 2341/3750 - LOSS: 2.0631258487701416\n",
      "\tBATCH 2361/3750 - LOSS: 1.7374579906463623\n",
      "\tBATCH 2381/3750 - LOSS: 1.8050414323806763\n",
      "\tBATCH 2401/3750 - LOSS: 2.4729106426239014\n",
      "\tBATCH 2421/3750 - LOSS: 2.0908966064453125\n",
      "\tBATCH 2441/3750 - LOSS: 2.069478988647461\n",
      "\tBATCH 2461/3750 - LOSS: 2.0866928100585938\n",
      "\tBATCH 2481/3750 - LOSS: 1.9695874452590942\n",
      "\tBATCH 2501/3750 - LOSS: 2.138740301132202\n",
      "\tBATCH 2521/3750 - LOSS: 1.8002873659133911\n",
      "\tBATCH 2541/3750 - LOSS: 2.0585057735443115\n",
      "\tBATCH 2561/3750 - LOSS: 1.966813325881958\n",
      "\tBATCH 2581/3750 - LOSS: 2.114781618118286\n",
      "\tBATCH 2601/3750 - LOSS: 2.1028640270233154\n",
      "\tBATCH 2621/3750 - LOSS: 1.8860290050506592\n",
      "\tBATCH 2641/3750 - LOSS: 1.8332395553588867\n",
      "\tBATCH 2661/3750 - LOSS: 1.7114135026931763\n",
      "\tBATCH 2681/3750 - LOSS: 1.8192477226257324\n",
      "\tBATCH 2701/3750 - LOSS: 1.8113995790481567\n",
      "\tBATCH 2721/3750 - LOSS: 1.9292117357254028\n",
      "\tBATCH 2741/3750 - LOSS: 1.8111885786056519\n",
      "\tBATCH 2761/3750 - LOSS: 1.7311537265777588\n",
      "\tBATCH 2781/3750 - LOSS: 1.8983979225158691\n",
      "\tBATCH 2801/3750 - LOSS: 1.7697609663009644\n",
      "\tBATCH 2821/3750 - LOSS: 1.9526035785675049\n",
      "\tBATCH 2841/3750 - LOSS: 2.020212411880493\n",
      "\tBATCH 2861/3750 - LOSS: 2.060530185699463\n",
      "\tBATCH 2881/3750 - LOSS: 2.166374921798706\n",
      "\tBATCH 2901/3750 - LOSS: 1.670076608657837\n",
      "\tBATCH 2921/3750 - LOSS: 1.6460363864898682\n",
      "\tBATCH 2941/3750 - LOSS: 2.1294002532958984\n",
      "\tBATCH 2961/3750 - LOSS: 2.302609443664551\n",
      "\tBATCH 2981/3750 - LOSS: 2.367645025253296\n",
      "\tBATCH 3001/3750 - LOSS: 2.0619540214538574\n",
      "\tBATCH 3021/3750 - LOSS: 1.954925537109375\n",
      "\tBATCH 3041/3750 - LOSS: 1.7773139476776123\n",
      "\tBATCH 3061/3750 - LOSS: 1.9424829483032227\n",
      "\tBATCH 3081/3750 - LOSS: 1.8719985485076904\n",
      "\tBATCH 3101/3750 - LOSS: 2.1141257286071777\n",
      "\tBATCH 3121/3750 - LOSS: 1.7028203010559082\n",
      "\tBATCH 3141/3750 - LOSS: 2.0961527824401855\n",
      "\tBATCH 3161/3750 - LOSS: 2.0528595447540283\n",
      "\tBATCH 3181/3750 - LOSS: 2.1856937408447266\n",
      "\tBATCH 3201/3750 - LOSS: 2.352912425994873\n",
      "\tBATCH 3221/3750 - LOSS: 1.8494874238967896\n",
      "\tBATCH 3241/3750 - LOSS: 2.656541347503662\n",
      "\tBATCH 3261/3750 - LOSS: 2.0983712673187256\n",
      "\tBATCH 3281/3750 - LOSS: 2.0279009342193604\n",
      "\tBATCH 3301/3750 - LOSS: 2.2424376010894775\n",
      "\tBATCH 3321/3750 - LOSS: 2.321258306503296\n",
      "\tBATCH 3341/3750 - LOSS: 2.1161317825317383\n",
      "\tBATCH 3361/3750 - LOSS: 2.013209819793701\n",
      "\tBATCH 3381/3750 - LOSS: 2.152191162109375\n",
      "\tBATCH 3401/3750 - LOSS: 2.4267776012420654\n",
      "\tBATCH 3421/3750 - LOSS: 2.518698215484619\n",
      "\tBATCH 3441/3750 - LOSS: 1.9410929679870605\n",
      "\tBATCH 3461/3750 - LOSS: 1.9313257932662964\n",
      "\tBATCH 3481/3750 - LOSS: 2.1538259983062744\n",
      "\tBATCH 3501/3750 - LOSS: 1.8848133087158203\n",
      "\tBATCH 3521/3750 - LOSS: 2.2891294956207275\n",
      "\tBATCH 3541/3750 - LOSS: 1.9243249893188477\n",
      "\tBATCH 3561/3750 - LOSS: 2.3652443885803223\n",
      "\tBATCH 3581/3750 - LOSS: 2.2670154571533203\n",
      "\tBATCH 3601/3750 - LOSS: 1.9625154733657837\n",
      "\tBATCH 3621/3750 - LOSS: 2.0722904205322266\n",
      "\tBATCH 3641/3750 - LOSS: 2.239053726196289\n",
      "\tBATCH 3661/3750 - LOSS: 1.9034278392791748\n",
      "\tBATCH 3681/3750 - LOSS: 1.8695909976959229\n",
      "\tBATCH 3701/3750 - LOSS: 1.7327322959899902\n",
      "\tBATCH 3721/3750 - LOSS: 1.745323896408081\n",
      "\tBATCH 3741/3750 - LOSS: 1.867799997329712\n",
      "\n",
      "\t[TRAIN] EPOCH 3 - LOSS: 1.9817781448364258, ACCURACY: 0.2734833359718323\n",
      "\n",
      "EPOCH 3 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.916005253791809, ACCURACY: 0.29089999198913574\n",
      "\n",
      "Validation loss decreased (2.2135 --> 1.9160).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 4 - TRAINING...\n",
      "\tBATCH 1/3750 - LOSS: 1.8264334201812744\n",
      "\tBATCH 21/3750 - LOSS: 1.7245038747787476\n",
      "\tBATCH 41/3750 - LOSS: 1.7876874208450317\n",
      "\tBATCH 61/3750 - LOSS: 1.4175083637237549\n",
      "\tBATCH 81/3750 - LOSS: 1.8064602613449097\n",
      "\tBATCH 101/3750 - LOSS: 1.727433443069458\n",
      "\tBATCH 121/3750 - LOSS: 2.113557815551758\n",
      "\tBATCH 141/3750 - LOSS: 2.0217533111572266\n",
      "\tBATCH 161/3750 - LOSS: 1.774343729019165\n",
      "\tBATCH 181/3750 - LOSS: 1.8895952701568604\n",
      "\tBATCH 201/3750 - LOSS: 1.6782090663909912\n",
      "\tBATCH 221/3750 - LOSS: 2.2652251720428467\n",
      "\tBATCH 241/3750 - LOSS: 2.0267958641052246\n",
      "\tBATCH 261/3750 - LOSS: 2.110841989517212\n",
      "\tBATCH 281/3750 - LOSS: 1.7762410640716553\n",
      "\tBATCH 301/3750 - LOSS: 1.5919519662857056\n",
      "\tBATCH 321/3750 - LOSS: 1.9293599128723145\n",
      "\tBATCH 341/3750 - LOSS: 2.1986048221588135\n",
      "\tBATCH 361/3750 - LOSS: 2.0821518898010254\n",
      "\tBATCH 381/3750 - LOSS: 2.068838357925415\n",
      "\tBATCH 401/3750 - LOSS: 1.6477899551391602\n",
      "\tBATCH 421/3750 - LOSS: 1.8167102336883545\n",
      "\tBATCH 441/3750 - LOSS: 1.8281776905059814\n",
      "\tBATCH 461/3750 - LOSS: 1.7668570280075073\n",
      "\tBATCH 481/3750 - LOSS: 2.0927038192749023\n",
      "\tBATCH 501/3750 - LOSS: 1.8560203313827515\n",
      "\tBATCH 521/3750 - LOSS: 1.8494373559951782\n",
      "\tBATCH 541/3750 - LOSS: 2.195356607437134\n",
      "\tBATCH 561/3750 - LOSS: 2.2274348735809326\n",
      "\tBATCH 581/3750 - LOSS: 1.7372874021530151\n",
      "\tBATCH 601/3750 - LOSS: 1.7324658632278442\n",
      "\tBATCH 621/3750 - LOSS: 2.044257164001465\n",
      "\tBATCH 641/3750 - LOSS: 2.197352409362793\n",
      "\tBATCH 661/3750 - LOSS: 1.6854751110076904\n",
      "\tBATCH 681/3750 - LOSS: 2.242506504058838\n",
      "\tBATCH 701/3750 - LOSS: 2.124584674835205\n",
      "\tBATCH 721/3750 - LOSS: 1.9449448585510254\n",
      "\tBATCH 741/3750 - LOSS: 1.9606685638427734\n",
      "\tBATCH 761/3750 - LOSS: 2.0315768718719482\n",
      "\tBATCH 781/3750 - LOSS: 1.7440649271011353\n",
      "\tBATCH 801/3750 - LOSS: 2.2534432411193848\n",
      "\tBATCH 821/3750 - LOSS: 2.052039623260498\n",
      "\tBATCH 841/3750 - LOSS: 1.712250828742981\n",
      "\tBATCH 861/3750 - LOSS: 2.374647617340088\n",
      "\tBATCH 881/3750 - LOSS: 1.7189949750900269\n",
      "\tBATCH 901/3750 - LOSS: 1.6246329545974731\n",
      "\tBATCH 921/3750 - LOSS: 2.0998995304107666\n",
      "\tBATCH 941/3750 - LOSS: 2.2849671840667725\n",
      "\tBATCH 961/3750 - LOSS: 2.0468950271606445\n",
      "\tBATCH 981/3750 - LOSS: 1.6552225351333618\n",
      "\tBATCH 1001/3750 - LOSS: 1.9760501384735107\n",
      "\tBATCH 1021/3750 - LOSS: 2.039024591445923\n",
      "\tBATCH 1041/3750 - LOSS: 1.4128915071487427\n",
      "\tBATCH 1061/3750 - LOSS: 1.9820667505264282\n",
      "\tBATCH 1081/3750 - LOSS: 1.8493530750274658\n",
      "\tBATCH 1101/3750 - LOSS: 1.3909543752670288\n",
      "\tBATCH 1121/3750 - LOSS: 1.8757247924804688\n",
      "\tBATCH 1141/3750 - LOSS: 1.4944932460784912\n",
      "\tBATCH 1161/3750 - LOSS: 1.7078648805618286\n",
      "\tBATCH 1181/3750 - LOSS: 1.7209287881851196\n",
      "\tBATCH 1201/3750 - LOSS: 1.4178762435913086\n",
      "\tBATCH 1221/3750 - LOSS: 1.6553868055343628\n",
      "\tBATCH 1241/3750 - LOSS: 1.8798010349273682\n",
      "\tBATCH 1261/3750 - LOSS: 1.5195773839950562\n",
      "\tBATCH 1281/3750 - LOSS: 1.7203723192214966\n",
      "\tBATCH 1301/3750 - LOSS: 1.7387151718139648\n",
      "\tBATCH 1321/3750 - LOSS: 1.4733045101165771\n",
      "\tBATCH 1341/3750 - LOSS: 1.7317620515823364\n",
      "\tBATCH 1361/3750 - LOSS: 1.960549235343933\n",
      "\tBATCH 1381/3750 - LOSS: 1.278198003768921\n",
      "\tBATCH 1401/3750 - LOSS: 1.519930124282837\n",
      "\tBATCH 1421/3750 - LOSS: 1.5348706245422363\n",
      "\tBATCH 1441/3750 - LOSS: 1.1138689517974854\n",
      "\tBATCH 1461/3750 - LOSS: 1.4798728227615356\n",
      "\tBATCH 1481/3750 - LOSS: 1.3143943548202515\n",
      "\tBATCH 1501/3750 - LOSS: 1.631318211555481\n",
      "\tBATCH 1521/3750 - LOSS: 1.4574663639068604\n",
      "\tBATCH 1541/3750 - LOSS: 1.8261160850524902\n",
      "\tBATCH 1561/3750 - LOSS: 1.7012633085250854\n",
      "\tBATCH 1581/3750 - LOSS: 1.3832892179489136\n",
      "\tBATCH 1601/3750 - LOSS: 1.783621072769165\n",
      "\tBATCH 1621/3750 - LOSS: 1.6220338344573975\n",
      "\tBATCH 1641/3750 - LOSS: 1.3915560245513916\n",
      "\tBATCH 1661/3750 - LOSS: 1.6788053512573242\n",
      "\tBATCH 1681/3750 - LOSS: 1.4537198543548584\n",
      "\tBATCH 1701/3750 - LOSS: 1.3670568466186523\n",
      "\tBATCH 1721/3750 - LOSS: 1.7313123941421509\n",
      "\tBATCH 1741/3750 - LOSS: 1.610435128211975\n",
      "\tBATCH 1761/3750 - LOSS: 2.718318223953247\n",
      "\tBATCH 1781/3750 - LOSS: 1.333795189857483\n",
      "\tBATCH 1801/3750 - LOSS: 1.8641732931137085\n",
      "\tBATCH 1821/3750 - LOSS: 1.3425613641738892\n",
      "\tBATCH 1841/3750 - LOSS: 1.2816574573516846\n",
      "\tBATCH 1861/3750 - LOSS: 1.7052513360977173\n",
      "\tBATCH 1881/3750 - LOSS: 1.6146750450134277\n",
      "\tBATCH 1901/3750 - LOSS: 1.9478182792663574\n",
      "\tBATCH 1921/3750 - LOSS: 2.220212936401367\n",
      "\tBATCH 1941/3750 - LOSS: 2.1002156734466553\n",
      "\tBATCH 1961/3750 - LOSS: 1.6180367469787598\n",
      "\tBATCH 1981/3750 - LOSS: 1.743872046470642\n",
      "\tBATCH 2001/3750 - LOSS: 1.14540433883667\n",
      "\tBATCH 2021/3750 - LOSS: 1.4129492044448853\n",
      "\tBATCH 2041/3750 - LOSS: 1.5475248098373413\n",
      "\tBATCH 2061/3750 - LOSS: 2.2198219299316406\n",
      "\tBATCH 2081/3750 - LOSS: 1.7767727375030518\n",
      "\tBATCH 2101/3750 - LOSS: 1.6240603923797607\n",
      "\tBATCH 2121/3750 - LOSS: 1.2626742124557495\n",
      "\tBATCH 2141/3750 - LOSS: 1.5777486562728882\n",
      "\tBATCH 2161/3750 - LOSS: 2.1597721576690674\n",
      "\tBATCH 2181/3750 - LOSS: 1.7397602796554565\n",
      "\tBATCH 2201/3750 - LOSS: 1.4612690210342407\n",
      "\tBATCH 2221/3750 - LOSS: 2.0144107341766357\n",
      "\tBATCH 2241/3750 - LOSS: 1.245968222618103\n",
      "\tBATCH 2261/3750 - LOSS: 1.69600510597229\n",
      "\tBATCH 2281/3750 - LOSS: 1.3386131525039673\n",
      "\tBATCH 2301/3750 - LOSS: 1.2442395687103271\n",
      "\tBATCH 2321/3750 - LOSS: 1.7054171562194824\n",
      "\tBATCH 2341/3750 - LOSS: 1.8654512166976929\n",
      "\tBATCH 2361/3750 - LOSS: 1.7595080137252808\n",
      "\tBATCH 2381/3750 - LOSS: 1.944260835647583\n",
      "\tBATCH 2401/3750 - LOSS: 1.509714126586914\n",
      "\tBATCH 2421/3750 - LOSS: 1.6668914556503296\n",
      "\tBATCH 2441/3750 - LOSS: 1.604554295539856\n",
      "\tBATCH 2461/3750 - LOSS: 1.6288375854492188\n",
      "\tBATCH 2481/3750 - LOSS: 1.4992681741714478\n",
      "\tBATCH 2501/3750 - LOSS: 2.0160257816314697\n",
      "\tBATCH 2521/3750 - LOSS: 1.4490717649459839\n",
      "\tBATCH 2541/3750 - LOSS: 1.3507708311080933\n",
      "\tBATCH 2561/3750 - LOSS: 1.459977149963379\n",
      "\tBATCH 2581/3750 - LOSS: 1.840512990951538\n",
      "\tBATCH 2601/3750 - LOSS: 1.5001899003982544\n",
      "\tBATCH 2621/3750 - LOSS: 1.6482598781585693\n",
      "\tBATCH 2641/3750 - LOSS: 1.9052629470825195\n",
      "\tBATCH 2661/3750 - LOSS: 1.0958411693572998\n",
      "\tBATCH 2681/3750 - LOSS: 1.3268868923187256\n",
      "\tBATCH 2701/3750 - LOSS: 1.4919543266296387\n",
      "\tBATCH 2721/3750 - LOSS: 1.7788662910461426\n",
      "\tBATCH 2741/3750 - LOSS: 1.1331074237823486\n",
      "\tBATCH 2761/3750 - LOSS: 1.4816869497299194\n",
      "\tBATCH 2781/3750 - LOSS: 1.6155486106872559\n",
      "\tBATCH 2801/3750 - LOSS: 1.6877273321151733\n",
      "\tBATCH 2821/3750 - LOSS: 1.0374022722244263\n",
      "\tBATCH 2841/3750 - LOSS: 1.6289089918136597\n",
      "\tBATCH 2861/3750 - LOSS: 1.507132649421692\n",
      "\tBATCH 2881/3750 - LOSS: 1.4658442735671997\n",
      "\tBATCH 2901/3750 - LOSS: 1.3602248430252075\n",
      "\tBATCH 2921/3750 - LOSS: 2.1803879737854004\n",
      "\tBATCH 2941/3750 - LOSS: 1.3064720630645752\n",
      "\tBATCH 2961/3750 - LOSS: 0.9687677621841431\n",
      "\tBATCH 2981/3750 - LOSS: 2.1548233032226562\n",
      "\tBATCH 3001/3750 - LOSS: 2.091752052307129\n",
      "\tBATCH 3021/3750 - LOSS: 1.9965547323226929\n",
      "\tBATCH 3041/3750 - LOSS: 1.6531333923339844\n",
      "\tBATCH 3061/3750 - LOSS: 1.7865808010101318\n",
      "\tBATCH 3081/3750 - LOSS: 1.9219441413879395\n",
      "\tBATCH 3101/3750 - LOSS: 1.7782090902328491\n",
      "\tBATCH 3121/3750 - LOSS: 1.7262132167816162\n",
      "\tBATCH 3141/3750 - LOSS: 1.30470609664917\n",
      "\tBATCH 3161/3750 - LOSS: 1.5753228664398193\n",
      "\tBATCH 3181/3750 - LOSS: 1.6154680252075195\n",
      "\tBATCH 3201/3750 - LOSS: 1.814180850982666\n",
      "\tBATCH 3221/3750 - LOSS: 1.7630181312561035\n",
      "\tBATCH 3241/3750 - LOSS: 1.993033528327942\n",
      "\tBATCH 3261/3750 - LOSS: 1.5933513641357422\n",
      "\tBATCH 3281/3750 - LOSS: 2.078731060028076\n",
      "\tBATCH 3301/3750 - LOSS: 1.7250608205795288\n",
      "\tBATCH 3321/3750 - LOSS: 2.085357427597046\n",
      "\tBATCH 3341/3750 - LOSS: 1.9432358741760254\n",
      "\tBATCH 3361/3750 - LOSS: 1.50470769405365\n",
      "\tBATCH 3381/3750 - LOSS: 1.7663054466247559\n",
      "\tBATCH 3401/3750 - LOSS: 1.3733271360397339\n",
      "\tBATCH 3421/3750 - LOSS: 1.2720059156417847\n",
      "\tBATCH 3441/3750 - LOSS: 1.4790129661560059\n",
      "\tBATCH 3461/3750 - LOSS: 1.388952374458313\n",
      "\tBATCH 3481/3750 - LOSS: 1.979478120803833\n",
      "\tBATCH 3501/3750 - LOSS: 1.722902774810791\n",
      "\tBATCH 3521/3750 - LOSS: 2.3995773792266846\n",
      "\tBATCH 3541/3750 - LOSS: 1.6418284177780151\n",
      "\tBATCH 3561/3750 - LOSS: 1.866045355796814\n",
      "\tBATCH 3581/3750 - LOSS: 1.0549770593643188\n",
      "\tBATCH 3601/3750 - LOSS: 1.9069468975067139\n",
      "\tBATCH 3621/3750 - LOSS: 2.0695462226867676\n",
      "\tBATCH 3641/3750 - LOSS: 1.6503942012786865\n",
      "\tBATCH 3661/3750 - LOSS: 1.7033722400665283\n",
      "\tBATCH 3681/3750 - LOSS: 2.2691726684570312\n",
      "\tBATCH 3701/3750 - LOSS: 1.8201769590377808\n",
      "\tBATCH 3721/3750 - LOSS: 1.592153549194336\n",
      "\tBATCH 3741/3750 - LOSS: 1.2791130542755127\n",
      "\n",
      "\t[TRAIN] EPOCH 4 - LOSS: 1.7139426469802856, ACCURACY: 0.39845001697540283\n",
      "\n",
      "EPOCH 4 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.5417653322219849, ACCURACY: 0.4544000029563904\n",
      "\n",
      "Validation loss decreased (1.9160 --> 1.5418).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 5 - TRAINING...\n",
      "\tBATCH 1/3750 - LOSS: 2.1671998500823975\n",
      "\tBATCH 21/3750 - LOSS: 1.665999174118042\n",
      "\tBATCH 41/3750 - LOSS: 1.6349626779556274\n",
      "\tBATCH 61/3750 - LOSS: 1.4379279613494873\n",
      "\tBATCH 81/3750 - LOSS: 1.589372992515564\n",
      "\tBATCH 101/3750 - LOSS: 1.341481328010559\n",
      "\tBATCH 121/3750 - LOSS: 1.4116231203079224\n",
      "\tBATCH 141/3750 - LOSS: 2.157813549041748\n",
      "\tBATCH 161/3750 - LOSS: 1.2637320756912231\n",
      "\tBATCH 181/3750 - LOSS: 1.0345332622528076\n",
      "\tBATCH 201/3750 - LOSS: 1.5008397102355957\n",
      "\tBATCH 221/3750 - LOSS: 1.5471199750900269\n",
      "\tBATCH 241/3750 - LOSS: 1.7121822834014893\n",
      "\tBATCH 261/3750 - LOSS: 1.9072644710540771\n",
      "\tBATCH 281/3750 - LOSS: 1.5929967164993286\n",
      "\tBATCH 301/3750 - LOSS: 1.3689953088760376\n",
      "\tBATCH 321/3750 - LOSS: 1.2406495809555054\n",
      "\tBATCH 341/3750 - LOSS: 2.315852165222168\n",
      "\tBATCH 361/3750 - LOSS: 1.3454256057739258\n",
      "\tBATCH 381/3750 - LOSS: 1.5088480710983276\n",
      "\tBATCH 401/3750 - LOSS: 1.870954990386963\n",
      "\tBATCH 421/3750 - LOSS: 1.656457781791687\n",
      "\tBATCH 441/3750 - LOSS: 1.6227867603302002\n",
      "\tBATCH 461/3750 - LOSS: 1.8039735555648804\n",
      "\tBATCH 481/3750 - LOSS: 1.898329496383667\n",
      "\tBATCH 501/3750 - LOSS: 1.4419065713882446\n",
      "\tBATCH 521/3750 - LOSS: 1.3796138763427734\n",
      "\tBATCH 541/3750 - LOSS: 1.7580698728561401\n",
      "\tBATCH 561/3750 - LOSS: 1.543949007987976\n",
      "\tBATCH 581/3750 - LOSS: 1.9787176847457886\n",
      "\tBATCH 601/3750 - LOSS: 2.3710639476776123\n",
      "\tBATCH 621/3750 - LOSS: 1.5139374732971191\n",
      "\tBATCH 641/3750 - LOSS: 1.2891943454742432\n",
      "\tBATCH 661/3750 - LOSS: 1.395921230316162\n",
      "\tBATCH 681/3750 - LOSS: 1.584650993347168\n",
      "\tBATCH 701/3750 - LOSS: 2.0004491806030273\n",
      "\tBATCH 721/3750 - LOSS: 1.8871535062789917\n",
      "\tBATCH 741/3750 - LOSS: 1.8158941268920898\n",
      "\tBATCH 761/3750 - LOSS: 1.5090036392211914\n",
      "\tBATCH 781/3750 - LOSS: 2.392115831375122\n",
      "\tBATCH 801/3750 - LOSS: 1.7606227397918701\n",
      "\tBATCH 821/3750 - LOSS: 1.260018229484558\n",
      "\tBATCH 841/3750 - LOSS: 1.4014400243759155\n",
      "\tBATCH 861/3750 - LOSS: 1.7812747955322266\n",
      "\tBATCH 881/3750 - LOSS: 1.7362724542617798\n",
      "\tBATCH 901/3750 - LOSS: 1.903459906578064\n",
      "\tBATCH 921/3750 - LOSS: 1.553245186805725\n",
      "\tBATCH 941/3750 - LOSS: 1.755621314048767\n",
      "\tBATCH 961/3750 - LOSS: 1.279248595237732\n",
      "\tBATCH 981/3750 - LOSS: 2.173184394836426\n",
      "\tBATCH 1001/3750 - LOSS: 1.8627102375030518\n",
      "\tBATCH 1021/3750 - LOSS: 1.7892327308654785\n",
      "\tBATCH 1041/3750 - LOSS: 1.8506708145141602\n",
      "\tBATCH 1061/3750 - LOSS: 1.691390872001648\n",
      "\tBATCH 1081/3750 - LOSS: 1.6488791704177856\n",
      "\tBATCH 1101/3750 - LOSS: 1.59652841091156\n",
      "\tBATCH 1121/3750 - LOSS: 1.4362393617630005\n",
      "\tBATCH 1141/3750 - LOSS: 1.3146681785583496\n",
      "\tBATCH 1161/3750 - LOSS: 1.550900936126709\n",
      "\tBATCH 1181/3750 - LOSS: 1.0881351232528687\n",
      "\tBATCH 1201/3750 - LOSS: 1.464680552482605\n",
      "\tBATCH 1221/3750 - LOSS: 2.0364062786102295\n",
      "\tBATCH 1241/3750 - LOSS: 1.6338741779327393\n",
      "\tBATCH 1261/3750 - LOSS: 1.511773705482483\n",
      "\tBATCH 1281/3750 - LOSS: 1.5921237468719482\n",
      "\tBATCH 1301/3750 - LOSS: 1.5980935096740723\n",
      "\tBATCH 1321/3750 - LOSS: 1.9034578800201416\n",
      "\tBATCH 1341/3750 - LOSS: 1.384294867515564\n",
      "\tBATCH 1361/3750 - LOSS: 1.6520873308181763\n",
      "\tBATCH 1381/3750 - LOSS: 1.3746068477630615\n",
      "\tBATCH 1401/3750 - LOSS: 1.6981337070465088\n",
      "\tBATCH 1421/3750 - LOSS: 1.4139765501022339\n",
      "\tBATCH 1441/3750 - LOSS: 1.746275544166565\n",
      "\tBATCH 1461/3750 - LOSS: 1.950307846069336\n",
      "\tBATCH 1481/3750 - LOSS: 1.358184576034546\n",
      "\tBATCH 1501/3750 - LOSS: 1.5902576446533203\n",
      "\tBATCH 1521/3750 - LOSS: 1.3094813823699951\n",
      "\tBATCH 1541/3750 - LOSS: 1.6286630630493164\n",
      "\tBATCH 1561/3750 - LOSS: 1.9996097087860107\n",
      "\tBATCH 1581/3750 - LOSS: 1.844157338142395\n",
      "\tBATCH 1601/3750 - LOSS: 1.2816656827926636\n",
      "\tBATCH 1621/3750 - LOSS: 1.586493730545044\n",
      "\tBATCH 1641/3750 - LOSS: 1.478264331817627\n",
      "\tBATCH 1661/3750 - LOSS: 1.6606003046035767\n",
      "\tBATCH 1681/3750 - LOSS: 1.4353699684143066\n",
      "\tBATCH 1701/3750 - LOSS: 1.7971134185791016\n",
      "\tBATCH 1721/3750 - LOSS: 1.5984913110733032\n",
      "\tBATCH 1741/3750 - LOSS: 1.5838876962661743\n",
      "\tBATCH 1761/3750 - LOSS: 1.7393176555633545\n",
      "\tBATCH 1781/3750 - LOSS: 2.090132474899292\n",
      "\tBATCH 1801/3750 - LOSS: 1.5484085083007812\n",
      "\tBATCH 1821/3750 - LOSS: 1.4186347723007202\n",
      "\tBATCH 1841/3750 - LOSS: 1.8993022441864014\n",
      "\tBATCH 1861/3750 - LOSS: 1.7245841026306152\n",
      "\tBATCH 1881/3750 - LOSS: 2.0071523189544678\n",
      "\tBATCH 1901/3750 - LOSS: 1.944898247718811\n",
      "\tBATCH 1921/3750 - LOSS: 1.188513159751892\n",
      "\tBATCH 1941/3750 - LOSS: 1.8079967498779297\n",
      "\tBATCH 1961/3750 - LOSS: 1.3955131769180298\n",
      "\tBATCH 1981/3750 - LOSS: 1.8055236339569092\n",
      "\tBATCH 2001/3750 - LOSS: 1.7635470628738403\n",
      "\tBATCH 2021/3750 - LOSS: 1.5144866704940796\n",
      "\tBATCH 2041/3750 - LOSS: 2.099715232849121\n",
      "\tBATCH 2061/3750 - LOSS: 1.857912540435791\n",
      "\tBATCH 2081/3750 - LOSS: 1.6235218048095703\n",
      "\tBATCH 2101/3750 - LOSS: 1.3084660768508911\n",
      "\tBATCH 2121/3750 - LOSS: 1.6328762769699097\n",
      "\tBATCH 2141/3750 - LOSS: 1.882928490638733\n",
      "\tBATCH 2161/3750 - LOSS: 1.5750943422317505\n",
      "\tBATCH 2181/3750 - LOSS: 1.8715931177139282\n",
      "\tBATCH 2201/3750 - LOSS: 1.6883714199066162\n",
      "\tBATCH 2221/3750 - LOSS: 2.0882768630981445\n",
      "\tBATCH 2241/3750 - LOSS: 1.5576046705245972\n",
      "\tBATCH 2261/3750 - LOSS: 2.2129833698272705\n",
      "\tBATCH 2281/3750 - LOSS: 1.4375512599945068\n",
      "\tBATCH 2301/3750 - LOSS: 1.0450851917266846\n",
      "\tBATCH 2321/3750 - LOSS: 1.2062793970108032\n",
      "\tBATCH 2341/3750 - LOSS: 1.5650088787078857\n",
      "\tBATCH 2361/3750 - LOSS: 1.501697063446045\n",
      "\tBATCH 2381/3750 - LOSS: 1.9116204977035522\n",
      "\tBATCH 2401/3750 - LOSS: 2.222590923309326\n",
      "\tBATCH 2421/3750 - LOSS: 1.6158874034881592\n",
      "\tBATCH 2441/3750 - LOSS: 1.3889070749282837\n",
      "\tBATCH 2461/3750 - LOSS: 1.9675407409667969\n",
      "\tBATCH 2481/3750 - LOSS: 2.0011048316955566\n",
      "\tBATCH 2501/3750 - LOSS: 1.9833006858825684\n",
      "\tBATCH 2521/3750 - LOSS: 1.8874553442001343\n",
      "\tBATCH 2541/3750 - LOSS: 1.6605992317199707\n",
      "\tBATCH 2561/3750 - LOSS: 1.947688102722168\n",
      "\tBATCH 2581/3750 - LOSS: 1.597105622291565\n",
      "\tBATCH 2601/3750 - LOSS: 1.7852427959442139\n",
      "\tBATCH 2621/3750 - LOSS: 1.76272451877594\n",
      "\tBATCH 2641/3750 - LOSS: 1.6408056020736694\n",
      "\tBATCH 2661/3750 - LOSS: 1.810954213142395\n",
      "\tBATCH 2681/3750 - LOSS: 1.4290010929107666\n",
      "\tBATCH 2701/3750 - LOSS: 1.9162923097610474\n",
      "\tBATCH 2721/3750 - LOSS: 1.6000672578811646\n",
      "\tBATCH 2741/3750 - LOSS: 1.6838884353637695\n",
      "\tBATCH 2761/3750 - LOSS: 2.2187929153442383\n",
      "\tBATCH 2781/3750 - LOSS: 1.552765130996704\n",
      "\tBATCH 2801/3750 - LOSS: 2.052680730819702\n",
      "\tBATCH 2821/3750 - LOSS: 1.9048726558685303\n",
      "\tBATCH 2841/3750 - LOSS: 1.9028995037078857\n",
      "\tBATCH 2861/3750 - LOSS: 1.3226655721664429\n",
      "\tBATCH 2881/3750 - LOSS: 2.0307319164276123\n",
      "\tBATCH 2901/3750 - LOSS: 1.9289873838424683\n",
      "\tBATCH 2921/3750 - LOSS: 1.1465632915496826\n",
      "\tBATCH 2941/3750 - LOSS: 1.7662274837493896\n",
      "\tBATCH 2961/3750 - LOSS: 1.7245562076568604\n",
      "\tBATCH 2981/3750 - LOSS: 1.581507921218872\n",
      "\tBATCH 3001/3750 - LOSS: 1.749314546585083\n",
      "\tBATCH 3021/3750 - LOSS: 1.6965895891189575\n",
      "\tBATCH 3041/3750 - LOSS: 2.181910753250122\n",
      "\tBATCH 3061/3750 - LOSS: 2.0287115573883057\n",
      "\tBATCH 3081/3750 - LOSS: 1.7971280813217163\n",
      "\tBATCH 3101/3750 - LOSS: 1.6125725507736206\n",
      "\tBATCH 3121/3750 - LOSS: 1.8604077100753784\n",
      "\tBATCH 3141/3750 - LOSS: 1.7235162258148193\n",
      "\tBATCH 3161/3750 - LOSS: 1.7579692602157593\n",
      "\tBATCH 3181/3750 - LOSS: 1.459324836730957\n",
      "\tBATCH 3201/3750 - LOSS: 2.0100419521331787\n",
      "\tBATCH 3221/3750 - LOSS: 1.5417780876159668\n",
      "\tBATCH 3241/3750 - LOSS: 1.9871187210083008\n",
      "\tBATCH 3261/3750 - LOSS: 1.7868154048919678\n",
      "\tBATCH 3281/3750 - LOSS: 1.7296943664550781\n",
      "\tBATCH 3301/3750 - LOSS: 1.5354101657867432\n",
      "\tBATCH 3321/3750 - LOSS: 1.7668529748916626\n",
      "\tBATCH 3341/3750 - LOSS: 1.6412328481674194\n",
      "\tBATCH 3361/3750 - LOSS: 1.4532339572906494\n",
      "\tBATCH 3381/3750 - LOSS: 1.6080143451690674\n",
      "\tBATCH 3401/3750 - LOSS: 1.8374333381652832\n",
      "\tBATCH 3421/3750 - LOSS: 1.3371423482894897\n",
      "\tBATCH 3441/3750 - LOSS: 1.557660698890686\n",
      "\tBATCH 3461/3750 - LOSS: 1.7414734363555908\n",
      "\tBATCH 3481/3750 - LOSS: 1.456180214881897\n",
      "\tBATCH 3501/3750 - LOSS: 1.500848650932312\n",
      "\tBATCH 3521/3750 - LOSS: 1.0631262063980103\n",
      "\tBATCH 3541/3750 - LOSS: 1.8033066987991333\n",
      "\tBATCH 3561/3750 - LOSS: 2.216622829437256\n",
      "\tBATCH 3581/3750 - LOSS: 1.341001272201538\n",
      "\tBATCH 3601/3750 - LOSS: 1.3743976354599\n",
      "\tBATCH 3621/3750 - LOSS: 1.2707103490829468\n",
      "\tBATCH 3641/3750 - LOSS: 1.3719797134399414\n",
      "\tBATCH 3661/3750 - LOSS: 1.7685387134552002\n",
      "\tBATCH 3681/3750 - LOSS: 1.3994534015655518\n",
      "\tBATCH 3701/3750 - LOSS: 1.3196781873703003\n",
      "\tBATCH 3721/3750 - LOSS: 1.5590641498565674\n",
      "\tBATCH 3741/3750 - LOSS: 1.4034961462020874\n",
      "\n",
      "\t[TRAIN] EPOCH 5 - LOSS: 1.691943645477295, ACCURACY: 0.39970001578330994\n",
      "\n",
      "EPOCH 5 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.720765233039856, ACCURACY: 0.37449997663497925\n",
      "\n",
      "==================================================\n",
      "EPOCH 6 - TRAINING...\n",
      "\tBATCH 1/3750 - LOSS: 1.4943305253982544\n",
      "\tBATCH 21/3750 - LOSS: 1.4979947805404663\n",
      "\tBATCH 41/3750 - LOSS: 1.6627452373504639\n",
      "\tBATCH 61/3750 - LOSS: 1.1226407289505005\n",
      "\tBATCH 81/3750 - LOSS: 1.1558544635772705\n",
      "\tBATCH 101/3750 - LOSS: 1.6444224119186401\n",
      "\tBATCH 121/3750 - LOSS: 1.5070401430130005\n",
      "\tBATCH 141/3750 - LOSS: 1.497396469116211\n",
      "\tBATCH 161/3750 - LOSS: 1.0848995447158813\n",
      "\tBATCH 181/3750 - LOSS: 1.5931462049484253\n",
      "\tBATCH 201/3750 - LOSS: 1.3493831157684326\n",
      "\tBATCH 221/3750 - LOSS: 1.7071917057037354\n",
      "\tBATCH 241/3750 - LOSS: 1.481438398361206\n",
      "\tBATCH 261/3750 - LOSS: 1.783105731010437\n",
      "\tBATCH 281/3750 - LOSS: 1.7059686183929443\n",
      "\tBATCH 301/3750 - LOSS: 1.7161827087402344\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ea0e599656ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-4ca4d18a84c2>\u001b[0m in \u001b[0;36mfit_tpu\u001b[0;34m(model, epochs, device, criterion, optimizer, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"EPOCH {epoch} - TRAINING...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         train_loss, train_acc = model.train_one_epoch(\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         )\n\u001b[1;32m     21\u001b[0m         print(\n",
      "\u001b[0;32m<ipython-input-9-fd481d369186>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mepoch_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                    maximize=group['maximize'])\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "lr = LR \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"INITIALIZING TRAINING\")\n",
    "start_time = datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "logs = fit_tpu(\n",
    "        model=model,\n",
    "        epochs=N_EPOCHS,\n",
    "        device=device,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "    )\n",
    "\n",
    "print(f\"Execution time: {datetime.now() - start_time}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PretrainedVIT_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1411a8a97d864de587e96fc3644834ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df95cdea5bd144df8b113f048cd2db40",
      "placeholder": "",
      "style": "IPY_MODEL_7364e5a2823e4058926254a187c93681",
      "value": " 170499072/? [00:02&lt;00:00, 88590067.11it/s]"
     }
    },
    "201b2082379744458a31a9665bd99f27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "26068d363c5b411cba6e8a07eb0eb335": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a9f79e02663b4b3a9624c4a4d30218ba",
       "IPY_MODEL_2f840aa0f4f44e518b8a644aebcd754d",
       "IPY_MODEL_1411a8a97d864de587e96fc3644834ae"
      ],
      "layout": "IPY_MODEL_d50974ca5be6429dafc3a20e4d63bb3d"
     }
    },
    "2f840aa0f4f44e518b8a644aebcd754d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46847522ffd24588bb4fec633bd7c3b1",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_201b2082379744458a31a9665bd99f27",
      "value": 170498071
     }
    },
    "35e96b7186764c07b620f68abb5e2b2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37285a82fc024ea493cf3619d88003cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "46847522ffd24588bb4fec633bd7c3b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7364e5a2823e4058926254a187c93681": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a9f79e02663b4b3a9624c4a4d30218ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35e96b7186764c07b620f68abb5e2b2e",
      "placeholder": "",
      "style": "IPY_MODEL_37285a82fc024ea493cf3619d88003cc",
      "value": ""
     }
    },
    "d50974ca5be6429dafc3a20e4d63bb3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df95cdea5bd144df8b113f048cd2db40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

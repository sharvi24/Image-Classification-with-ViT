{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQmN-D2okUmU",
    "outputId": "70aa490a-52a6-42f0-8630-fffaa60e65e1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import timm\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import model_selection, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VISGgteanFpk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PretrainedVIT_CIFAR_Neha.ipynb',\n",
       " 'torch-1.7-cp37-cp37m-linux_x86_64.whl',\n",
       " 'torch_xla-1.7-cp37-cp37m-linux_x86_64.whl',\n",
       " 'cassava-leaf-disease-classification',\n",
       " 'pytorch-xla-env-setup.py',\n",
       " '.jovianrc',\n",
       " 'Fashion_MNIST.ipynb',\n",
       " 'jx_vit_base_p16_224-80ecf9dd.pth',\n",
       " '.ipynb_checkpoints',\n",
       " 'PretrainedVIT_AS_Cassava.ipynb',\n",
       " 'Pretrained.ipynb',\n",
       " 'CIFAR10.ipynb',\n",
       " 'torchvision-1.7-cp37-cp37m-linux_x86_64.whl',\n",
       " 'PretrainedVIT_CIFAR.ipynb',\n",
       " 'Cassava.ipynb']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vSmJeUwc_nd"
   },
   "source": [
    "## CASSAVA Final - 40 eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "RO09IiLFnOIC"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = (\"jx_vit_base_p16_224-80ecf9dd.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "aJHrfoV5et7j"
   },
   "outputs": [],
   "source": [
    "# For parallelization in TPUs\n",
    "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
    "os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GysE0Vf9dDJ8"
   },
   "outputs": [],
   "source": [
    "# general global variables\n",
    "DATA_PATH = \"cassava-leaf-disease-classification\"\n",
    "TRAIN_PATH = \"cassava-leaf-disease-classification/train_images\"\n",
    "TEST_PATH = \"cassava-leaf-disease-classification/test_images\"\n",
    "MODEL_PATH = (\"jx_vit_base_p16_224-80ecf9dd.pth\")\n",
    "# model specific global variables\n",
    "\n",
    "## Cassava\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.001\n",
    "# GAMMA = 0.7\n",
    "N_EPOCHS = 40 # 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cassava-leaf-disease-classification/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "gCMlnRm6dw46"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CassavaDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Helper Class to create the pytorch dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, data_path=DATA_PATH, mode=\"train\", transforms=None):\n",
    "        super().__init__()\n",
    "        self.df_data = df.values\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        self.data_dir = \"train_images\" if mode == \"train\" else \"test_images\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name, label = self.df_data[index]\n",
    "        img_path = os.path.join(self.data_path, self.data_dir, img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(img)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "v19_qDdjdt70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = model_selection.train_test_split(\n",
    "    df, test_size=0.1, random_state=42, stratify=df.label.values\n",
    ")\n",
    "\n",
    "# create image augmentations\n",
    "transformations = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),transforms.ToTensor()])\n",
    "\n",
    "train_dataset = CassavaDataset(train_df, transforms=transformations)\n",
    "valid_dataset = CassavaDataset(valid_df, transforms=transformations)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,drop_last=True,num_workers=8)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_size=BATCH_SIZE,drop_last=True,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BYHIPJM5d2dq"
   },
   "outputs": [],
   "source": [
    "class ViTBase16(nn.Module):\n",
    "    def __init__(self, n_classes, pretrained=False):\n",
    "\n",
    "        super(ViTBase16, self).__init__()\n",
    "\n",
    "        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained=False)\n",
    "        if pretrained:\n",
    "            self.model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n",
    "        # keep track of training loss\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        self.model.train()\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            if device.type == \"cuda\":\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = self.forward(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # Calculate Accuracy\n",
    "            accuracy = (output.argmax(dim=1) == target).float().mean()\n",
    "            # update training loss and accuracy\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "            optimizer.step()\n",
    "\n",
    "            # perform a single optimization step (parameter update)\n",
    "           \n",
    "            if i % 20 == 0:\n",
    "                print(f\"\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss}\")\n",
    "                \n",
    "\n",
    "        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\n",
    "\n",
    "    def validate_one_epoch(self, valid_loader, criterion, device):\n",
    "        # keep track of validation loss\n",
    "        valid_loss = 0.0\n",
    "        valid_accuracy = 0.0\n",
    "\n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        self.model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if device.type == \"cuda\":\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = self.model(data)\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, target)\n",
    "                # Calculate Accuracy\n",
    "                accuracy = (output.argmax(dim=1) == target).float().mean()\n",
    "                # update average validation loss and accuracy\n",
    "                valid_loss += loss\n",
    "                valid_accuracy += accuracy\n",
    "\n",
    "        return valid_loss / len(valid_loader), valid_accuracy / len(valid_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "F7Wlso0ueFTD"
   },
   "outputs": [],
   "source": [
    "def fit_tpu(\n",
    "    model, epochs, device, criterion, optimizer, train_loader, valid_loader=None\n",
    "):\n",
    "\n",
    "    valid_loss_min = np.Inf  # track change in validation loss\n",
    "\n",
    "    # keeping track of losses as it happen\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"EPOCH {epoch} - TRAINING...\")\n",
    "        train_loss, train_acc = model.train_one_epoch(\n",
    "            train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        print(\n",
    "            f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\n\"\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        gc.collect()\n",
    "\n",
    "        if valid_loader is not None:\n",
    "            gc.collect()\n",
    "            print(f\"EPOCH {epoch} - VALIDATING...\")\n",
    "            valid_loss, valid_acc = model.validate_one_epoch(\n",
    "                valid_loader, criterion, device\n",
    "            )\n",
    "            print(f\"\\t[VALID] LOSS: {valid_loss}, ACCURACY: {valid_acc}\\n\")\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_accs.append(valid_acc)\n",
    "            gc.collect()\n",
    "\n",
    "            # save model if validation loss has decreased\n",
    "            if valid_loss <= valid_loss_min and epoch != 1:\n",
    "                print(\n",
    "                    \"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\".format(\n",
    "                        valid_loss_min, valid_loss\n",
    "                    )\n",
    "                )\n",
    "            #                 xm.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"valid_losses\": valid_losses,\n",
    "        \"train_acc\": train_accs,\n",
    "        \"valid_acc\": valid_accs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4o2PUpUheJCP"
   },
   "outputs": [],
   "source": [
    "model = ViTBase16(n_classes=5, pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_UiD9YweNxX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING TRAINING\n",
      "Start Time: 2022-05-12 18:47:15.680916\n",
      "==================================================\n",
      "EPOCH 1 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 1.715251088142395\n",
      "\tBATCH 21/1203 - LOSS: 1.236167073249817\n",
      "\tBATCH 41/1203 - LOSS: 1.166198968887329\n",
      "\tBATCH 61/1203 - LOSS: 1.0795202255249023\n",
      "\tBATCH 81/1203 - LOSS: 1.6125942468643188\n",
      "\tBATCH 101/1203 - LOSS: 0.6729816794395447\n",
      "\tBATCH 121/1203 - LOSS: 1.1487425565719604\n",
      "\tBATCH 141/1203 - LOSS: 1.504636526107788\n",
      "\tBATCH 161/1203 - LOSS: 0.7722253203392029\n",
      "\tBATCH 181/1203 - LOSS: 1.3899345397949219\n",
      "\tBATCH 201/1203 - LOSS: 1.0432389974594116\n",
      "\tBATCH 221/1203 - LOSS: 0.9115486145019531\n",
      "\tBATCH 241/1203 - LOSS: 1.0663762092590332\n",
      "\tBATCH 261/1203 - LOSS: 1.1596888303756714\n",
      "\tBATCH 281/1203 - LOSS: 1.1666537523269653\n",
      "\tBATCH 301/1203 - LOSS: 1.0621743202209473\n",
      "\tBATCH 321/1203 - LOSS: 1.0519338846206665\n",
      "\tBATCH 341/1203 - LOSS: 1.5109314918518066\n",
      "\tBATCH 361/1203 - LOSS: 1.0512527227401733\n",
      "\tBATCH 381/1203 - LOSS: 1.3005682229995728\n",
      "\tBATCH 401/1203 - LOSS: 1.1964164972305298\n",
      "\tBATCH 421/1203 - LOSS: 1.3476580381393433\n",
      "\tBATCH 441/1203 - LOSS: 1.020780086517334\n",
      "\tBATCH 461/1203 - LOSS: 1.1454555988311768\n",
      "\tBATCH 481/1203 - LOSS: 1.4401448965072632\n",
      "\tBATCH 501/1203 - LOSS: 1.342036247253418\n",
      "\tBATCH 521/1203 - LOSS: 1.4425101280212402\n",
      "\tBATCH 541/1203 - LOSS: 1.117044448852539\n",
      "\tBATCH 561/1203 - LOSS: 1.5408003330230713\n",
      "\tBATCH 581/1203 - LOSS: 1.3125615119934082\n",
      "\tBATCH 601/1203 - LOSS: 0.8215953707695007\n",
      "\tBATCH 621/1203 - LOSS: 1.1457383632659912\n",
      "\tBATCH 641/1203 - LOSS: 0.7705701589584351\n",
      "\tBATCH 661/1203 - LOSS: 2.0379467010498047\n",
      "\tBATCH 681/1203 - LOSS: 0.9889984726905823\n",
      "\tBATCH 701/1203 - LOSS: 0.8174903392791748\n",
      "\tBATCH 721/1203 - LOSS: 1.2424572706222534\n",
      "\tBATCH 741/1203 - LOSS: 1.4058337211608887\n",
      "\tBATCH 761/1203 - LOSS: 1.1322855949401855\n",
      "\tBATCH 781/1203 - LOSS: 1.101042628288269\n",
      "\tBATCH 801/1203 - LOSS: 1.0958093404769897\n",
      "\tBATCH 821/1203 - LOSS: 1.0779000520706177\n",
      "\tBATCH 841/1203 - LOSS: 1.045015811920166\n",
      "\tBATCH 861/1203 - LOSS: 1.1896415948867798\n",
      "\tBATCH 881/1203 - LOSS: 1.1879663467407227\n",
      "\tBATCH 901/1203 - LOSS: 0.900169312953949\n",
      "\tBATCH 921/1203 - LOSS: 1.115936517715454\n",
      "\tBATCH 941/1203 - LOSS: 1.3385616540908813\n",
      "\tBATCH 961/1203 - LOSS: 1.3013166189193726\n",
      "\tBATCH 981/1203 - LOSS: 0.8358625769615173\n",
      "\tBATCH 1001/1203 - LOSS: 1.3058744668960571\n",
      "\tBATCH 1021/1203 - LOSS: 1.0316261053085327\n",
      "\tBATCH 1041/1203 - LOSS: 1.0536885261535645\n",
      "\tBATCH 1061/1203 - LOSS: 1.1365039348602295\n",
      "\tBATCH 1081/1203 - LOSS: 1.0781497955322266\n",
      "\tBATCH 1101/1203 - LOSS: 1.1035184860229492\n",
      "\tBATCH 1121/1203 - LOSS: 1.2215921878814697\n",
      "\tBATCH 1141/1203 - LOSS: 1.1532459259033203\n",
      "\tBATCH 1161/1203 - LOSS: 1.1379802227020264\n",
      "\tBATCH 1181/1203 - LOSS: 1.1115810871124268\n",
      "\tBATCH 1201/1203 - LOSS: 0.8966305255889893\n",
      "\n",
      "\t[TRAIN] EPOCH 1 - LOSS: 1.197130560874939, ACCURACY: 0.6141417622566223\n",
      "\n",
      "EPOCH 1 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.1856348514556885, ACCURACY: 0.6151316165924072\n",
      "\n",
      "==================================================\n",
      "EPOCH 2 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 1.4380449056625366\n",
      "\tBATCH 21/1203 - LOSS: 1.307337760925293\n",
      "\tBATCH 41/1203 - LOSS: 1.1549829244613647\n",
      "\tBATCH 61/1203 - LOSS: 1.0768777132034302\n",
      "\tBATCH 81/1203 - LOSS: 1.5451674461364746\n",
      "\tBATCH 101/1203 - LOSS: 0.6610380411148071\n",
      "\tBATCH 121/1203 - LOSS: 1.1094945669174194\n",
      "\tBATCH 141/1203 - LOSS: 1.5187265872955322\n",
      "\tBATCH 161/1203 - LOSS: 0.8054793477058411\n",
      "\tBATCH 181/1203 - LOSS: 1.3647767305374146\n",
      "\tBATCH 201/1203 - LOSS: 1.0370492935180664\n",
      "\tBATCH 221/1203 - LOSS: 0.9351956248283386\n",
      "\tBATCH 241/1203 - LOSS: 1.0614666938781738\n",
      "\tBATCH 261/1203 - LOSS: 1.133359432220459\n",
      "\tBATCH 281/1203 - LOSS: 1.1593358516693115\n",
      "\tBATCH 301/1203 - LOSS: 1.0625197887420654\n",
      "\tBATCH 321/1203 - LOSS: 1.0340484380722046\n",
      "\tBATCH 341/1203 - LOSS: 1.523810863494873\n",
      "\tBATCH 361/1203 - LOSS: 1.0607688426971436\n",
      "\tBATCH 381/1203 - LOSS: 1.2375633716583252\n",
      "\tBATCH 401/1203 - LOSS: 1.2273378372192383\n",
      "\tBATCH 421/1203 - LOSS: 1.3109288215637207\n",
      "\tBATCH 441/1203 - LOSS: 1.0176739692687988\n",
      "\tBATCH 461/1203 - LOSS: 1.1518263816833496\n",
      "\tBATCH 481/1203 - LOSS: 1.4225571155548096\n",
      "\tBATCH 501/1203 - LOSS: 1.304628610610962\n",
      "\tBATCH 521/1203 - LOSS: 1.4438544511795044\n",
      "\tBATCH 541/1203 - LOSS: 1.106564998626709\n",
      "\tBATCH 561/1203 - LOSS: 1.5677708387374878\n",
      "\tBATCH 581/1203 - LOSS: 1.248956322669983\n",
      "\tBATCH 601/1203 - LOSS: 0.8239072561264038\n",
      "\tBATCH 621/1203 - LOSS: 1.1454602479934692\n",
      "\tBATCH 641/1203 - LOSS: 0.7687745690345764\n",
      "\tBATCH 661/1203 - LOSS: 2.0358662605285645\n",
      "\tBATCH 681/1203 - LOSS: 0.9937033653259277\n",
      "\tBATCH 701/1203 - LOSS: 0.8241381049156189\n",
      "\tBATCH 721/1203 - LOSS: 1.308227300643921\n",
      "\tBATCH 741/1203 - LOSS: 1.4192684888839722\n",
      "\tBATCH 761/1203 - LOSS: 1.1715251207351685\n",
      "\tBATCH 781/1203 - LOSS: 1.0614101886749268\n",
      "\tBATCH 801/1203 - LOSS: 1.0988768339157104\n",
      "\tBATCH 821/1203 - LOSS: 1.0723254680633545\n",
      "\tBATCH 841/1203 - LOSS: 1.030603051185608\n",
      "\tBATCH 861/1203 - LOSS: 1.1371570825576782\n",
      "\tBATCH 881/1203 - LOSS: 1.1647659540176392\n",
      "\tBATCH 901/1203 - LOSS: 0.8580074310302734\n",
      "\tBATCH 921/1203 - LOSS: 1.0513814687728882\n",
      "\tBATCH 941/1203 - LOSS: 1.3182235956192017\n",
      "\tBATCH 961/1203 - LOSS: 1.3125511407852173\n",
      "\tBATCH 981/1203 - LOSS: 0.8444324731826782\n",
      "\tBATCH 1001/1203 - LOSS: 1.3193581104278564\n",
      "\tBATCH 1021/1203 - LOSS: 1.0050734281539917\n",
      "\tBATCH 1041/1203 - LOSS: 1.0241835117340088\n",
      "\tBATCH 1061/1203 - LOSS: 0.9769775867462158\n",
      "\tBATCH 1081/1203 - LOSS: 1.0505435466766357\n",
      "\tBATCH 1101/1203 - LOSS: 1.0487769842147827\n",
      "\tBATCH 1121/1203 - LOSS: 1.0870721340179443\n",
      "\tBATCH 1141/1203 - LOSS: 1.0961562395095825\n",
      "\tBATCH 1161/1203 - LOSS: 1.1938141584396362\n",
      "\tBATCH 1181/1203 - LOSS: 1.1720777750015259\n",
      "\tBATCH 1201/1203 - LOSS: 0.8860310316085815\n",
      "\n",
      "\t[TRAIN] EPOCH 2 - LOSS: 1.1780036687850952, ACCURACY: 0.6151807904243469\n",
      "\n",
      "EPOCH 2 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.1974921226501465, ACCURACY: 0.6151316165924072\n",
      "\n",
      "==================================================\n",
      "EPOCH 3 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 1.495287299156189\n",
      "\tBATCH 21/1203 - LOSS: 1.2795895338058472\n",
      "\tBATCH 41/1203 - LOSS: 1.1454463005065918\n",
      "\tBATCH 61/1203 - LOSS: 1.0870753526687622\n",
      "\tBATCH 81/1203 - LOSS: 1.5768544673919678\n",
      "\tBATCH 101/1203 - LOSS: 0.6372050046920776\n",
      "\tBATCH 121/1203 - LOSS: 1.149269700050354\n",
      "\tBATCH 141/1203 - LOSS: 1.533322811126709\n",
      "\tBATCH 161/1203 - LOSS: 0.7846224308013916\n",
      "\tBATCH 181/1203 - LOSS: 1.378048062324524\n",
      "\tBATCH 201/1203 - LOSS: 0.9614728689193726\n",
      "\tBATCH 221/1203 - LOSS: 0.9286144375801086\n",
      "\tBATCH 241/1203 - LOSS: 1.0889623165130615\n",
      "\tBATCH 261/1203 - LOSS: 0.9470182657241821\n",
      "\tBATCH 281/1203 - LOSS: 1.1467812061309814\n",
      "\tBATCH 301/1203 - LOSS: 0.9326794147491455\n",
      "\tBATCH 321/1203 - LOSS: 0.9597038626670837\n",
      "\tBATCH 341/1203 - LOSS: 1.3266946077346802\n",
      "\tBATCH 361/1203 - LOSS: 1.1340525150299072\n",
      "\tBATCH 381/1203 - LOSS: 1.1601732969284058\n",
      "\tBATCH 401/1203 - LOSS: 1.1233887672424316\n",
      "\tBATCH 421/1203 - LOSS: 1.1718957424163818\n",
      "\tBATCH 441/1203 - LOSS: 0.9790237545967102\n",
      "\tBATCH 461/1203 - LOSS: 1.1337690353393555\n",
      "\tBATCH 481/1203 - LOSS: 1.5584371089935303\n",
      "\tBATCH 501/1203 - LOSS: 1.3031737804412842\n",
      "\tBATCH 521/1203 - LOSS: 1.3181477785110474\n",
      "\tBATCH 541/1203 - LOSS: 1.0035253763198853\n",
      "\tBATCH 561/1203 - LOSS: 1.4690016508102417\n",
      "\tBATCH 581/1203 - LOSS: 1.2939115762710571\n",
      "\tBATCH 601/1203 - LOSS: 0.7753992080688477\n",
      "\tBATCH 621/1203 - LOSS: 1.0366027355194092\n",
      "\tBATCH 641/1203 - LOSS: 0.8809670805931091\n",
      "\tBATCH 661/1203 - LOSS: 1.7185883522033691\n",
      "\tBATCH 681/1203 - LOSS: 1.0213308334350586\n",
      "\tBATCH 701/1203 - LOSS: 0.620009183883667\n",
      "\tBATCH 721/1203 - LOSS: 1.2545568943023682\n",
      "\tBATCH 741/1203 - LOSS: 1.3486528396606445\n",
      "\tBATCH 761/1203 - LOSS: 1.1119247674942017\n",
      "\tBATCH 781/1203 - LOSS: 1.0072330236434937\n",
      "\tBATCH 801/1203 - LOSS: 0.749638557434082\n",
      "\tBATCH 821/1203 - LOSS: 1.1172720193862915\n",
      "\tBATCH 841/1203 - LOSS: 0.92094886302948\n",
      "\tBATCH 861/1203 - LOSS: 1.0374646186828613\n",
      "\tBATCH 881/1203 - LOSS: 1.057781457901001\n",
      "\tBATCH 901/1203 - LOSS: 0.7385910749435425\n",
      "\tBATCH 921/1203 - LOSS: 1.1043422222137451\n",
      "\tBATCH 941/1203 - LOSS: 1.1885324716567993\n",
      "\tBATCH 961/1203 - LOSS: 1.1381009817123413\n",
      "\tBATCH 981/1203 - LOSS: 0.8165796995162964\n",
      "\tBATCH 1001/1203 - LOSS: 1.1449153423309326\n",
      "\tBATCH 1021/1203 - LOSS: 0.9213457703590393\n",
      "\tBATCH 1041/1203 - LOSS: 0.9461329579353333\n",
      "\tBATCH 1061/1203 - LOSS: 0.757686197757721\n",
      "\tBATCH 1081/1203 - LOSS: 1.0309025049209595\n",
      "\tBATCH 1101/1203 - LOSS: 0.9806991219520569\n",
      "\tBATCH 1121/1203 - LOSS: 1.0351516008377075\n",
      "\tBATCH 1141/1203 - LOSS: 1.0063400268554688\n",
      "\tBATCH 1161/1203 - LOSS: 1.1125900745391846\n",
      "\tBATCH 1181/1203 - LOSS: 1.1426876783370972\n",
      "\tBATCH 1201/1203 - LOSS: 0.7669512033462524\n",
      "\n",
      "\t[TRAIN] EPOCH 3 - LOSS: 1.1229726076126099, ACCURACY: 0.6212074160575867\n",
      "\n",
      "EPOCH 3 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.1142337322235107, ACCURACY: 0.6240601539611816\n",
      "\n",
      "Validation loss decreased (1.1975 --> 1.1142).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 4 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 1.375429391860962\n",
      "\tBATCH 21/1203 - LOSS: 1.1474924087524414\n",
      "\tBATCH 41/1203 - LOSS: 0.9711747169494629\n",
      "\tBATCH 61/1203 - LOSS: 1.2540500164031982\n",
      "\tBATCH 81/1203 - LOSS: 1.2297661304473877\n",
      "\tBATCH 101/1203 - LOSS: 0.6464699506759644\n",
      "\tBATCH 121/1203 - LOSS: 1.219215989112854\n",
      "\tBATCH 141/1203 - LOSS: 1.3852615356445312\n",
      "\tBATCH 161/1203 - LOSS: 0.6924285888671875\n",
      "\tBATCH 181/1203 - LOSS: 1.3102138042449951\n",
      "\tBATCH 201/1203 - LOSS: 0.9665414094924927\n",
      "\tBATCH 221/1203 - LOSS: 1.0241526365280151\n",
      "\tBATCH 241/1203 - LOSS: 0.9599842429161072\n",
      "\tBATCH 261/1203 - LOSS: 0.9021086096763611\n",
      "\tBATCH 281/1203 - LOSS: 1.156190276145935\n",
      "\tBATCH 301/1203 - LOSS: 0.87882399559021\n",
      "\tBATCH 321/1203 - LOSS: 1.0294044017791748\n",
      "\tBATCH 341/1203 - LOSS: 1.3048714399337769\n",
      "\tBATCH 361/1203 - LOSS: 1.021034598350525\n",
      "\tBATCH 381/1203 - LOSS: 1.0788487195968628\n",
      "\tBATCH 401/1203 - LOSS: 1.0348893404006958\n",
      "\tBATCH 421/1203 - LOSS: 1.2135753631591797\n",
      "\tBATCH 441/1203 - LOSS: 0.9000951647758484\n",
      "\tBATCH 461/1203 - LOSS: 1.0411533117294312\n",
      "\tBATCH 481/1203 - LOSS: 1.5510692596435547\n",
      "\tBATCH 501/1203 - LOSS: 1.2061148881912231\n",
      "\tBATCH 521/1203 - LOSS: 1.2621580362319946\n",
      "\tBATCH 541/1203 - LOSS: 0.9488936066627502\n",
      "\tBATCH 561/1203 - LOSS: 1.4687504768371582\n",
      "\tBATCH 581/1203 - LOSS: 1.289546012878418\n",
      "\tBATCH 601/1203 - LOSS: 0.7099668979644775\n",
      "\tBATCH 621/1203 - LOSS: 0.9220367670059204\n",
      "\tBATCH 641/1203 - LOSS: 0.8489858508110046\n",
      "\tBATCH 661/1203 - LOSS: 1.6875524520874023\n",
      "\tBATCH 681/1203 - LOSS: 0.8978976011276245\n",
      "\tBATCH 701/1203 - LOSS: 0.5952244400978088\n",
      "\tBATCH 721/1203 - LOSS: 1.1207365989685059\n",
      "\tBATCH 741/1203 - LOSS: 1.3247841596603394\n",
      "\tBATCH 761/1203 - LOSS: 1.0684064626693726\n",
      "\tBATCH 781/1203 - LOSS: 1.0869865417480469\n",
      "\tBATCH 801/1203 - LOSS: 0.717418909072876\n",
      "\tBATCH 821/1203 - LOSS: 1.1643167734146118\n",
      "\tBATCH 841/1203 - LOSS: 0.9246460199356079\n",
      "\tBATCH 861/1203 - LOSS: 1.0890263319015503\n",
      "\tBATCH 881/1203 - LOSS: 0.9709819555282593\n",
      "\tBATCH 901/1203 - LOSS: 0.7812283039093018\n",
      "\tBATCH 921/1203 - LOSS: 1.00632905960083\n",
      "\tBATCH 941/1203 - LOSS: 1.1753075122833252\n",
      "\tBATCH 961/1203 - LOSS: 1.119208574295044\n",
      "\tBATCH 981/1203 - LOSS: 0.8016957640647888\n",
      "\tBATCH 1001/1203 - LOSS: 1.11573326587677\n",
      "\tBATCH 1021/1203 - LOSS: 0.9091821312904358\n",
      "\tBATCH 1041/1203 - LOSS: 0.8875918984413147\n",
      "\tBATCH 1061/1203 - LOSS: 0.7844535708427429\n",
      "\tBATCH 1081/1203 - LOSS: 0.9852651953697205\n",
      "\tBATCH 1101/1203 - LOSS: 1.0092984437942505\n",
      "\tBATCH 1121/1203 - LOSS: 1.1211938858032227\n",
      "\tBATCH 1141/1203 - LOSS: 0.9905526041984558\n",
      "\tBATCH 1161/1203 - LOSS: 1.0744938850402832\n",
      "\tBATCH 1181/1203 - LOSS: 1.021153211593628\n",
      "\tBATCH 1201/1203 - LOSS: 0.8206878900527954\n",
      "\n",
      "\t[TRAIN] EPOCH 4 - LOSS: 1.0789703130722046, ACCURACY: 0.6296238899230957\n",
      "\n",
      "EPOCH 4 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.0710272789001465, ACCURACY: 0.6329887509346008\n",
      "\n",
      "Validation loss decreased (1.1142 --> 1.0710).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 5 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 1.2309833765029907\n",
      "\tBATCH 21/1203 - LOSS: 1.1531051397323608\n",
      "\tBATCH 41/1203 - LOSS: 0.905378520488739\n",
      "\tBATCH 61/1203 - LOSS: 1.2404581308364868\n",
      "\tBATCH 81/1203 - LOSS: 1.1350154876708984\n",
      "\tBATCH 101/1203 - LOSS: 0.6029165983200073\n",
      "\tBATCH 121/1203 - LOSS: 1.3220598697662354\n",
      "\tBATCH 141/1203 - LOSS: 1.3630714416503906\n",
      "\tBATCH 161/1203 - LOSS: 0.6762277483940125\n",
      "\tBATCH 181/1203 - LOSS: 1.3314430713653564\n",
      "\tBATCH 201/1203 - LOSS: 0.9803397059440613\n",
      "\tBATCH 221/1203 - LOSS: 1.0296680927276611\n",
      "\tBATCH 241/1203 - LOSS: 1.0146114826202393\n",
      "\tBATCH 261/1203 - LOSS: 0.884798526763916\n",
      "\tBATCH 281/1203 - LOSS: 1.1537342071533203\n",
      "\tBATCH 301/1203 - LOSS: 0.8881856203079224\n",
      "\tBATCH 321/1203 - LOSS: 0.9310829043388367\n",
      "\tBATCH 341/1203 - LOSS: 1.3005187511444092\n",
      "\tBATCH 361/1203 - LOSS: 0.9283715486526489\n",
      "\tBATCH 381/1203 - LOSS: 1.0007272958755493\n",
      "\tBATCH 401/1203 - LOSS: 0.9789797067642212\n",
      "\tBATCH 421/1203 - LOSS: 1.310948133468628\n",
      "\tBATCH 441/1203 - LOSS: 0.914145827293396\n",
      "\tBATCH 461/1203 - LOSS: 1.0914090871810913\n",
      "\tBATCH 481/1203 - LOSS: 1.4976799488067627\n",
      "\tBATCH 501/1203 - LOSS: 1.0783401727676392\n",
      "\tBATCH 521/1203 - LOSS: 1.2544337511062622\n",
      "\tBATCH 541/1203 - LOSS: 0.8012542128562927\n",
      "\tBATCH 561/1203 - LOSS: 1.4998902082443237\n",
      "\tBATCH 581/1203 - LOSS: 1.182305097579956\n",
      "\tBATCH 601/1203 - LOSS: 0.5910273790359497\n",
      "\tBATCH 621/1203 - LOSS: 0.9313445091247559\n",
      "\tBATCH 641/1203 - LOSS: 0.8340075612068176\n",
      "\tBATCH 661/1203 - LOSS: 1.4195252656936646\n",
      "\tBATCH 681/1203 - LOSS: 0.8184412121772766\n",
      "\tBATCH 701/1203 - LOSS: 0.6713278293609619\n",
      "\tBATCH 721/1203 - LOSS: 1.0685182809829712\n",
      "\tBATCH 741/1203 - LOSS: 1.3033512830734253\n",
      "\tBATCH 761/1203 - LOSS: 0.993454098701477\n",
      "\tBATCH 781/1203 - LOSS: 1.048505187034607\n",
      "\tBATCH 801/1203 - LOSS: 0.7171400785446167\n",
      "\tBATCH 821/1203 - LOSS: 1.0876407623291016\n",
      "\tBATCH 841/1203 - LOSS: 0.9245700836181641\n",
      "\tBATCH 861/1203 - LOSS: 1.0986080169677734\n",
      "\tBATCH 881/1203 - LOSS: 0.9732279777526855\n",
      "\tBATCH 901/1203 - LOSS: 0.8487560153007507\n",
      "\tBATCH 921/1203 - LOSS: 0.9248431921005249\n",
      "\tBATCH 941/1203 - LOSS: 1.1472092866897583\n",
      "\tBATCH 961/1203 - LOSS: 1.149918794631958\n",
      "\tBATCH 981/1203 - LOSS: 0.7432543039321899\n",
      "\tBATCH 1001/1203 - LOSS: 1.0783146619796753\n",
      "\tBATCH 1021/1203 - LOSS: 0.9103949666023254\n",
      "\tBATCH 1041/1203 - LOSS: 0.8520774245262146\n",
      "\tBATCH 1061/1203 - LOSS: 0.8995336890220642\n",
      "\tBATCH 1081/1203 - LOSS: 0.8748670220375061\n",
      "\tBATCH 1101/1203 - LOSS: 0.9096163511276245\n",
      "\tBATCH 1121/1203 - LOSS: 1.0165458917617798\n",
      "\tBATCH 1141/1203 - LOSS: 0.8694408535957336\n",
      "\tBATCH 1161/1203 - LOSS: 1.1271015405654907\n",
      "\tBATCH 1181/1203 - LOSS: 0.9543032050132751\n",
      "\tBATCH 1201/1203 - LOSS: 0.7551454305648804\n",
      "\n",
      "\t[TRAIN] EPOCH 5 - LOSS: 1.0492793321609497, ACCURACY: 0.6327930092811584\n",
      "\n",
      "EPOCH 5 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.0720024108886719, ACCURACY: 0.6306391358375549\n",
      "\n",
      "==================================================\n",
      "EPOCH 6 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 1.1578088998794556\n",
      "\tBATCH 21/1203 - LOSS: 1.1879956722259521\n",
      "\tBATCH 41/1203 - LOSS: 0.9146603345870972\n",
      "\tBATCH 61/1203 - LOSS: 1.193719506263733\n",
      "\tBATCH 81/1203 - LOSS: 1.1674367189407349\n",
      "\tBATCH 101/1203 - LOSS: 0.5649041533470154\n",
      "\tBATCH 121/1203 - LOSS: 1.0531854629516602\n",
      "\tBATCH 141/1203 - LOSS: 1.2342889308929443\n",
      "\tBATCH 161/1203 - LOSS: 0.6310718655586243\n",
      "\tBATCH 181/1203 - LOSS: 1.5118211507797241\n",
      "\tBATCH 201/1203 - LOSS: 0.8392419815063477\n",
      "\tBATCH 221/1203 - LOSS: 0.9545707702636719\n",
      "\tBATCH 241/1203 - LOSS: 1.0306615829467773\n",
      "\tBATCH 261/1203 - LOSS: 0.8706164956092834\n",
      "\tBATCH 281/1203 - LOSS: 1.0792502164840698\n",
      "\tBATCH 301/1203 - LOSS: 0.8552964329719543\n",
      "\tBATCH 321/1203 - LOSS: 0.8096126317977905\n",
      "\tBATCH 341/1203 - LOSS: 1.339167594909668\n",
      "\tBATCH 361/1203 - LOSS: 0.9062400460243225\n",
      "\tBATCH 381/1203 - LOSS: 0.9294370412826538\n",
      "\tBATCH 401/1203 - LOSS: 0.9571699500083923\n",
      "\tBATCH 421/1203 - LOSS: 1.358322262763977\n",
      "\tBATCH 441/1203 - LOSS: 0.8709084987640381\n",
      "\tBATCH 461/1203 - LOSS: 1.0433063507080078\n",
      "\tBATCH 481/1203 - LOSS: 1.4158374071121216\n",
      "\tBATCH 501/1203 - LOSS: 0.9421151876449585\n",
      "\tBATCH 521/1203 - LOSS: 1.2599751949310303\n",
      "\tBATCH 541/1203 - LOSS: 0.7133786678314209\n",
      "\tBATCH 561/1203 - LOSS: 1.396795630455017\n",
      "\tBATCH 581/1203 - LOSS: 1.0788688659667969\n",
      "\tBATCH 601/1203 - LOSS: 0.5192921757698059\n",
      "\tBATCH 621/1203 - LOSS: 0.904725193977356\n",
      "\tBATCH 641/1203 - LOSS: 0.9006118178367615\n",
      "\tBATCH 661/1203 - LOSS: 1.2944629192352295\n",
      "\tBATCH 681/1203 - LOSS: 0.6895381808280945\n",
      "\tBATCH 701/1203 - LOSS: 0.5798757672309875\n",
      "\tBATCH 721/1203 - LOSS: 1.0278935432434082\n",
      "\tBATCH 741/1203 - LOSS: 1.2529914379119873\n",
      "\tBATCH 761/1203 - LOSS: 1.0025216341018677\n",
      "\tBATCH 781/1203 - LOSS: 1.0099799633026123\n",
      "\tBATCH 801/1203 - LOSS: 0.5907138586044312\n",
      "\tBATCH 821/1203 - LOSS: 1.0114772319793701\n",
      "\tBATCH 841/1203 - LOSS: 0.9643881320953369\n",
      "\tBATCH 861/1203 - LOSS: 1.1343376636505127\n",
      "\tBATCH 881/1203 - LOSS: 0.8992988467216492\n",
      "\tBATCH 901/1203 - LOSS: 0.6559932231903076\n",
      "\tBATCH 921/1203 - LOSS: 0.863963782787323\n",
      "\tBATCH 941/1203 - LOSS: 1.0804487466812134\n",
      "\tBATCH 961/1203 - LOSS: 1.1109933853149414\n",
      "\tBATCH 981/1203 - LOSS: 0.548602819442749\n",
      "\tBATCH 1001/1203 - LOSS: 1.099517822265625\n",
      "\tBATCH 1021/1203 - LOSS: 0.8535032272338867\n",
      "\tBATCH 1041/1203 - LOSS: 0.8414537310600281\n",
      "\tBATCH 1061/1203 - LOSS: 0.8436645269393921\n",
      "\tBATCH 1081/1203 - LOSS: 0.7837640643119812\n",
      "\tBATCH 1101/1203 - LOSS: 0.8704177737236023\n",
      "\tBATCH 1121/1203 - LOSS: 0.8295239806175232\n",
      "\tBATCH 1141/1203 - LOSS: 0.8353326320648193\n",
      "\tBATCH 1161/1203 - LOSS: 1.2402679920196533\n",
      "\tBATCH 1181/1203 - LOSS: 0.9680881500244141\n",
      "\tBATCH 1201/1203 - LOSS: 0.6333911418914795\n",
      "\n",
      "\t[TRAIN] EPOCH 6 - LOSS: 1.0051449537277222, ACCURACY: 0.6435993313789368\n",
      "\n",
      "EPOCH 6 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.0285922288894653, ACCURACY: 0.647086501121521\n",
      "\n",
      "Validation loss decreased (1.0720 --> 1.0286).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 7 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 1.130100131034851\n",
      "\tBATCH 21/1203 - LOSS: 1.1202391386032104\n",
      "\tBATCH 41/1203 - LOSS: 0.9440962672233582\n",
      "\tBATCH 61/1203 - LOSS: 1.1254723072052002\n",
      "\tBATCH 81/1203 - LOSS: 0.9659402966499329\n",
      "\tBATCH 101/1203 - LOSS: 0.5300095081329346\n",
      "\tBATCH 121/1203 - LOSS: 1.0369584560394287\n",
      "\tBATCH 141/1203 - LOSS: 1.2233197689056396\n",
      "\tBATCH 161/1203 - LOSS: 0.554843008518219\n",
      "\tBATCH 181/1203 - LOSS: 1.326535701751709\n",
      "\tBATCH 201/1203 - LOSS: 0.7697315216064453\n",
      "\tBATCH 221/1203 - LOSS: 0.8699237108230591\n",
      "\tBATCH 241/1203 - LOSS: 0.9335471987724304\n",
      "\tBATCH 261/1203 - LOSS: 0.6681857705116272\n",
      "\tBATCH 281/1203 - LOSS: 1.0636014938354492\n",
      "\tBATCH 301/1203 - LOSS: 0.8698451519012451\n",
      "\tBATCH 321/1203 - LOSS: 0.7730455994606018\n",
      "\tBATCH 341/1203 - LOSS: 1.1723381280899048\n",
      "\tBATCH 361/1203 - LOSS: 0.8158146739006042\n",
      "\tBATCH 381/1203 - LOSS: 0.8144357204437256\n",
      "\tBATCH 401/1203 - LOSS: 0.9652379751205444\n",
      "\tBATCH 421/1203 - LOSS: 1.3071448802947998\n",
      "\tBATCH 441/1203 - LOSS: 0.9010907411575317\n",
      "\tBATCH 461/1203 - LOSS: 1.0311601161956787\n",
      "\tBATCH 481/1203 - LOSS: 1.2995102405548096\n",
      "\tBATCH 501/1203 - LOSS: 0.9760594964027405\n",
      "\tBATCH 521/1203 - LOSS: 1.1386585235595703\n",
      "\tBATCH 541/1203 - LOSS: 0.6488178968429565\n",
      "\tBATCH 561/1203 - LOSS: 1.3851677179336548\n",
      "\tBATCH 581/1203 - LOSS: 0.9630041122436523\n",
      "\tBATCH 601/1203 - LOSS: 0.4993993043899536\n",
      "\tBATCH 621/1203 - LOSS: 0.8661679625511169\n",
      "\tBATCH 641/1203 - LOSS: 0.9573132991790771\n",
      "\tBATCH 661/1203 - LOSS: 1.214123010635376\n",
      "\tBATCH 681/1203 - LOSS: 0.6899697184562683\n",
      "\tBATCH 701/1203 - LOSS: 0.6075211763381958\n",
      "\tBATCH 721/1203 - LOSS: 0.9803804159164429\n",
      "\tBATCH 741/1203 - LOSS: 1.1560477018356323\n",
      "\tBATCH 761/1203 - LOSS: 0.8970078229904175\n",
      "\tBATCH 781/1203 - LOSS: 1.0144319534301758\n",
      "\tBATCH 801/1203 - LOSS: 0.5876224040985107\n",
      "\tBATCH 821/1203 - LOSS: 0.9042642116546631\n",
      "\tBATCH 841/1203 - LOSS: 0.7241361737251282\n",
      "\tBATCH 861/1203 - LOSS: 0.910104513168335\n",
      "\tBATCH 881/1203 - LOSS: 0.8807246685028076\n",
      "\tBATCH 901/1203 - LOSS: 0.5699394345283508\n",
      "\tBATCH 921/1203 - LOSS: 0.8460541367530823\n",
      "\tBATCH 941/1203 - LOSS: 1.0388377904891968\n",
      "\tBATCH 961/1203 - LOSS: 1.0603312253952026\n",
      "\tBATCH 981/1203 - LOSS: 0.5542384386062622\n",
      "\tBATCH 1001/1203 - LOSS: 0.9543687701225281\n",
      "\tBATCH 1021/1203 - LOSS: 0.7710946798324585\n",
      "\tBATCH 1041/1203 - LOSS: 0.749487578868866\n",
      "\tBATCH 1061/1203 - LOSS: 0.7511786222457886\n",
      "\tBATCH 1081/1203 - LOSS: 0.7606940269470215\n",
      "\tBATCH 1101/1203 - LOSS: 0.9029445052146912\n",
      "\tBATCH 1121/1203 - LOSS: 0.9142444133758545\n",
      "\tBATCH 1141/1203 - LOSS: 0.9008674621582031\n",
      "\tBATCH 1161/1203 - LOSS: 0.9650782942771912\n",
      "\tBATCH 1181/1203 - LOSS: 0.9502190351486206\n",
      "\tBATCH 1201/1203 - LOSS: 0.6017748117446899\n",
      "\n",
      "\t[TRAIN] EPOCH 7 - LOSS: 0.9580950140953064, ACCURACY: 0.6511325836181641\n",
      "\n",
      "EPOCH 7 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.9663242101669312, ACCURACY: 0.6461466550827026\n",
      "\n",
      "Validation loss decreased (1.0286 --> 0.9663).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 8 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 1.0143760442733765\n",
      "\tBATCH 21/1203 - LOSS: 1.0582013130187988\n",
      "\tBATCH 41/1203 - LOSS: 1.042281150817871\n",
      "\tBATCH 61/1203 - LOSS: 1.1206305027008057\n",
      "\tBATCH 81/1203 - LOSS: 0.9904283285140991\n",
      "\tBATCH 101/1203 - LOSS: 0.47889047861099243\n",
      "\tBATCH 121/1203 - LOSS: 0.9794580936431885\n",
      "\tBATCH 141/1203 - LOSS: 1.0826534032821655\n",
      "\tBATCH 161/1203 - LOSS: 0.47196513414382935\n",
      "\tBATCH 181/1203 - LOSS: 1.2336418628692627\n",
      "\tBATCH 201/1203 - LOSS: 0.7672556638717651\n",
      "\tBATCH 221/1203 - LOSS: 0.7210702896118164\n",
      "\tBATCH 241/1203 - LOSS: 0.8245044946670532\n",
      "\tBATCH 261/1203 - LOSS: 0.7035040855407715\n",
      "\tBATCH 281/1203 - LOSS: 0.9711170196533203\n",
      "\tBATCH 301/1203 - LOSS: 0.9126799702644348\n",
      "\tBATCH 321/1203 - LOSS: 0.6666167378425598\n",
      "\tBATCH 341/1203 - LOSS: 1.164623498916626\n",
      "\tBATCH 361/1203 - LOSS: 0.7531701326370239\n",
      "\tBATCH 381/1203 - LOSS: 0.6995765566825867\n",
      "\tBATCH 401/1203 - LOSS: 0.8548420071601868\n",
      "\tBATCH 421/1203 - LOSS: 1.2865020036697388\n",
      "\tBATCH 441/1203 - LOSS: 0.8260758519172668\n",
      "\tBATCH 461/1203 - LOSS: 1.0119291543960571\n",
      "\tBATCH 481/1203 - LOSS: 1.3510606288909912\n",
      "\tBATCH 501/1203 - LOSS: 0.9711505770683289\n",
      "\tBATCH 521/1203 - LOSS: 1.1207900047302246\n",
      "\tBATCH 541/1203 - LOSS: 0.6554892063140869\n",
      "\tBATCH 561/1203 - LOSS: 1.3793879747390747\n",
      "\tBATCH 581/1203 - LOSS: 0.9561334252357483\n",
      "\tBATCH 601/1203 - LOSS: 0.5326917767524719\n",
      "\tBATCH 621/1203 - LOSS: 0.8572374582290649\n",
      "\tBATCH 641/1203 - LOSS: 0.9007202386856079\n",
      "\tBATCH 661/1203 - LOSS: 1.1248390674591064\n",
      "\tBATCH 681/1203 - LOSS: 0.6116892099380493\n",
      "\tBATCH 701/1203 - LOSS: 0.5966448783874512\n",
      "\tBATCH 721/1203 - LOSS: 0.8909711837768555\n",
      "\tBATCH 741/1203 - LOSS: 1.1140621900558472\n",
      "\tBATCH 761/1203 - LOSS: 0.8080820441246033\n",
      "\tBATCH 781/1203 - LOSS: 1.03830885887146\n",
      "\tBATCH 801/1203 - LOSS: 0.5729580521583557\n",
      "\tBATCH 821/1203 - LOSS: 0.7478616833686829\n",
      "\tBATCH 841/1203 - LOSS: 0.6222283840179443\n",
      "\tBATCH 861/1203 - LOSS: 1.068688154220581\n",
      "\tBATCH 881/1203 - LOSS: 0.7304978370666504\n",
      "\tBATCH 901/1203 - LOSS: 0.46379929780960083\n",
      "\tBATCH 921/1203 - LOSS: 0.7544474601745605\n",
      "\tBATCH 941/1203 - LOSS: 1.1550692319869995\n",
      "\tBATCH 961/1203 - LOSS: 1.043851375579834\n",
      "\tBATCH 981/1203 - LOSS: 0.5605786442756653\n",
      "\tBATCH 1001/1203 - LOSS: 0.9626134634017944\n",
      "\tBATCH 1021/1203 - LOSS: 0.8658662438392639\n",
      "\tBATCH 1041/1203 - LOSS: 0.758901834487915\n",
      "\tBATCH 1061/1203 - LOSS: 0.9022732377052307\n",
      "\tBATCH 1081/1203 - LOSS: 0.6520771384239197\n",
      "\tBATCH 1101/1203 - LOSS: 0.9045555591583252\n",
      "\tBATCH 1121/1203 - LOSS: 0.7631869316101074\n",
      "\tBATCH 1141/1203 - LOSS: 0.9223992824554443\n",
      "\tBATCH 1161/1203 - LOSS: 0.8935548663139343\n",
      "\tBATCH 1181/1203 - LOSS: 0.8428452610969543\n",
      "\tBATCH 1201/1203 - LOSS: 0.5097764134407043\n",
      "\n",
      "\t[TRAIN] EPOCH 8 - LOSS: 0.9323130249977112, ACCURACY: 0.6593412756919861\n",
      "\n",
      "EPOCH 8 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.9634237885475159, ACCURACY: 0.6536654233932495\n",
      "\n",
      "Validation loss decreased (0.9663 --> 0.9634).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 9 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 0.9636400938034058\n",
      "\tBATCH 21/1203 - LOSS: 0.9520555734634399\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "\n",
    "lr = LR \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"INITIALIZING TRAINING\")\n",
    "start_time = datetime.now()\n",
    "print(f\"Start Time: {start_time}\")\n",
    "\n",
    "logs = fit_tpu(\n",
    "        model=model,\n",
    "        epochs=N_EPOCHS,\n",
    "        device=device,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "    )\n",
    "\n",
    "print(f\"Execution time: {datetime.now() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJuusMAcsQ3G"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PretrainedVIT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn import model_selection, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"cassava-leaf-disease-classification\"\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "train_df, valid_df = model_selection.train_test_split(\n",
    "    df, test_size=0.1, random_state=42, stratify=df.label.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassavaDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Helper Class to create the pytorch dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, data_path=DATA_PATH, mode=\"train\", transforms=None):\n",
    "        super().__init__()\n",
    "        self.df_data = df.values\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        self.data_dir = \"train_images\" if mode == \"train\" else \"test_images\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name, label = self.df_data[index]\n",
    "        img_path = os.path.join(self.data_path, self.data_dir, img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(img)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image augmentations\n",
    "IMG_SIZE = 224\n",
    "transforms_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "        transforms.RandomResizedCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_valid = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CassavaDataset(train_df, transforms=transforms_train)\n",
    "valid_dataset = CassavaDataset(valid_df, transforms=transforms_valid)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        drop_last=True,\n",
    "        num_workers=8,\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        drop_last=True,\n",
    "        num_workers=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, input_shape, n_patches=14, hidden=8, num_heads=2, n_classes=5):\n",
    "        # Super constructor\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Input and patches sizes\n",
    "        self.input_shape = input_shape\n",
    "        self.n_patches = n_patches\n",
    "        self.num_heads = num_heads\n",
    "        self.patch_size = (input_shape[1] / n_patches, input_shape[2] / n_patches)\n",
    "        self.hidden = hidden\n",
    "        #single patch input\n",
    "        self.input = int(input_shape[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear = nn.Linear(self.input, self.hidden)\n",
    "\n",
    "        # Classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden))\n",
    "        \n",
    "        #Layer norms\n",
    "        self.ln1 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden))\n",
    "        self.ln2 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden))\n",
    "        #Attention layer\n",
    "        self.attn = Attention(self.hidden, num_heads)\n",
    "        #MLP Encoder\n",
    "        self.mlpenc = nn.Sequential(\n",
    "            nn.Linear(self.hidden, self.hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden, n_classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        B, C, W, H = images.shape\n",
    "        patches = images.reshape(B, self.n_patches ** 2, self.input)\n",
    "        tokens = self.linear(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "        # Adding positional embedding\n",
    "        tokens += get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden).repeat(B, 1, 1)\n",
    "        \n",
    "        output = tokens + self.attn(self.ln1(tokens))\n",
    "        output = output + self.mlpenc(self.ln2(output))\n",
    "\n",
    "        # Getting the classification token only\n",
    "        output = output[:, 0]\n",
    "\n",
    "        return self.mlp(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d, num_heads=2):\n",
    "        super(Attention, self).__init__()\n",
    "        self.d = d\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        d_head = int(d / num_heads)\n",
    "        self.q_mappings = [nn.Linear(d_head, d_head) for _ in range(self.num_heads)]\n",
    "        self.k_mappings = [nn.Linear(d_head, d_head) for _ in range(self.num_heads)]\n",
    "        self.v_mappings = [nn.Linear(d_head, d_head) for _ in range(self.num_heads)]\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.num_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Defining model and training options\n",
    "    n_channels = 3\n",
    "    IMG_SIZE = 224\n",
    "    model = ViT((n_channels, IMG_SIZE, IMG_SIZE), n_patches=14, hidden=20, num_heads=2, n_classes=5)\n",
    "    N_EPOCHS = 40\n",
    "    LR = 0.001\n",
    "\n",
    "    # Training loop\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        accuracy, total = 0, 0\n",
    "        for batch in train_loader:\n",
    "            image, label = batch\n",
    "            pred = model(image)\n",
    "            loss = criterion(pred, label) / len(image)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            accuracy += torch.sum(torch.argmax(pred, dim=1) == label).item()\n",
    "            total += len(image)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}  Train Accuracy :{accuracy / total:.2f}\")\n",
    "\n",
    "    # Test loop\n",
    "    accuracy, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in test_loader:\n",
    "        image, label = batch\n",
    "        pred = model(image)\n",
    "        loss = criterion(pred, label) / len(image)\n",
    "        test_loss += loss\n",
    "\n",
    "        accuracy += torch.sum(torch.argmax(pred, dim=1) == label).item()\n",
    "        total += len(image)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {accuracy / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

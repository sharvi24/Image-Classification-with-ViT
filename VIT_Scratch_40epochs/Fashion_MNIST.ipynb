{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f607818d9d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, input_shape, n_patches=7, hidden=8, num_heads=2, n_classes=10):\n",
    "        # Super constructor\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Input and patches sizes\n",
    "        self.input_shape = input_shape\n",
    "        self.n_patches = n_patches\n",
    "        self.num_heads = num_heads\n",
    "        assert input_shape[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert input_shape[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (input_shape[1] / n_patches, input_shape[2] / n_patches)\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input = int(input_shape[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input, self.hidden)\n",
    "\n",
    "        # 2) Classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "        # (In forward method)\n",
    "\n",
    "        # 4a) Layer normalization 1\n",
    "        self.ln1 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden))\n",
    "\n",
    "        # 4b) Multi-head Self Attention (MSA) and classification token\n",
    "        self.attn = Attention(self.hidden, num_heads)\n",
    "\n",
    "        # 5a) Layer normalization 2\n",
    "        self.ln2 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden))\n",
    "\n",
    "        # 5b) Encoder MLP\n",
    "        self.mlpenc = nn.Sequential(\n",
    "            nn.Linear(self.hidden, self.hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 6) Classification MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden, n_classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        n, c, w, h = images.shape\n",
    "        patches = images.reshape(n, self.n_patches ** 2, self.input)\n",
    "\n",
    "        # Running linear layer for tokenization\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "        # Adding positional embedding\n",
    "        tokens += get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden).repeat(n, 1, 1)\n",
    "\n",
    "        # TRANSFORMER ENCODER BEGINS ###################################\n",
    "        # NOTICE: MULTIPLE ENCODER BLOCKS CAN BE STACKED TOGETHER ######\n",
    "        # Running Layer Normalization, MSA and residual connection\n",
    "        output = tokens + self.attn(self.ln1(tokens))\n",
    "\n",
    "        # Running Layer Normalization, MLP and residual connection\n",
    "        output = output + self.mlpenc(self.ln2(output))\n",
    "        # TRANSFORMER ENCODER ENDS   ###################################\n",
    "\n",
    "        # Getting the classification token only\n",
    "        output = output[:, 0]\n",
    "\n",
    "        return self.mlp(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d, num_heads=2):\n",
    "        super(Attention, self).__init__()\n",
    "        self.d = d\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        #assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / num_heads)\n",
    "        self.q_mappings = [nn.Linear(d_head, d_head) for _ in range(self.num_heads)]\n",
    "        self.k_mappings = [nn.Linear(d_head, d_head) for _ in range(self.num_heads)]\n",
    "        self.v_mappings = [nn.Linear(d_head, d_head) for _ in range(self.num_heads)]\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.num_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Loading data\n",
    "    transform = ToTensor()\n",
    "\n",
    "    train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)\n",
    "    test_set = MNIST(root='./../datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_set, shuffle=True, batch_size=16)\n",
    "    test_loader = DataLoader(test_set, shuffle=False, batch_size=16)\n",
    "\n",
    "    # Defining model and training options\n",
    "    model = ViT((1, 28, 28), n_patches=7, hidden=20, num_heads=2, n_classes=10)\n",
    "    N_EPOCHS = 40\n",
    "    LR = 0.01\n",
    "\n",
    "    # Training loop\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            x, y = batch\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y) / len(x)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "    # Test loop\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y) / len(x)\n",
    "        test_loss += loss\n",
    "\n",
    "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).item()\n",
    "        total += len(x)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 loss: 397.68\n",
      "Epoch 2/40 loss: 367.24\n",
      "Epoch 3/40 loss: 364.39\n",
      "Epoch 4/40 loss: 362.56\n",
      "Epoch 5/40 loss: 361.24\n",
      "Epoch 6/40 loss: 360.83\n",
      "Epoch 7/40 loss: 360.00\n",
      "Epoch 8/40 loss: 360.40\n",
      "Epoch 9/40 loss: 359.67\n",
      "Epoch 10/40 loss: 360.20\n",
      "Epoch 11/40 loss: 359.72\n",
      "Epoch 12/40 loss: 359.72\n",
      "Epoch 13/40 loss: 359.93\n",
      "Epoch 14/40 loss: 358.94\n",
      "Epoch 15/40 loss: 359.03\n",
      "Epoch 16/40 loss: 359.08\n",
      "Epoch 17/40 loss: 359.84\n",
      "Epoch 18/40 loss: 359.87\n",
      "Epoch 19/40 loss: 359.36\n",
      "Epoch 20/40 loss: 359.75\n",
      "Epoch 21/40 loss: 360.22\n",
      "Epoch 22/40 loss: 359.55\n",
      "Epoch 23/40 loss: 359.47\n",
      "Epoch 24/40 loss: 359.36\n",
      "Epoch 25/40 loss: 359.59\n",
      "Epoch 26/40 loss: 359.32\n",
      "Epoch 27/40 loss: 359.58\n",
      "Epoch 28/40 loss: 359.75\n",
      "Epoch 29/40 loss: 360.12\n",
      "Epoch 30/40 loss: 359.26\n",
      "Epoch 31/40 loss: 359.55\n",
      "Epoch 32/40 loss: 360.28\n",
      "Epoch 33/40 loss: 359.89\n",
      "Epoch 34/40 loss: 359.73\n",
      "Epoch 35/40 loss: 360.14\n",
      "Epoch 36/40 loss: 360.43\n",
      "Epoch 37/40 loss: 359.72\n",
      "Epoch 38/40 loss: 360.27\n",
      "Epoch 39/40 loss: 359.84\n",
      "Epoch 40/40 loss: 359.40\n",
      "Test loss: 59.75\n",
      "Test accuracy: 93.09%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
